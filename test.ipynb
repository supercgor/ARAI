{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "offset = 1\n",
    "select_num = 0\n",
    "upper = 4\n",
    "a = list(range(10))\n",
    "r = sample(range(upper), k=(select_num % upper))\n",
    "[i + offset for i in range(upper) for _ in range(select_num // upper + (i in r))]\n",
    "a[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def weighted_split(nums, ratio, mode = math.ceil):\n",
    "        if len(ratio) <= 1:\n",
    "            return nums,\n",
    "        else:\n",
    "            out = mode(nums * ratio[0]/sum(ratio))\n",
    "            return out, *weighted_split(nums - out, ratio = ratio[1:], mode = mode)\n",
    "\n",
    "weighted_split(100, (0.33,0.42,0.17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 999\n",
    "p = (0.3,0.3,0.3)\n",
    "print([a * i / sum(p) for i in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "a.extend(i for i in range(10))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset import indexGen\n",
    "import os\n",
    "path = \"/home/supercgor/gitfile/ARAI/datasets/data/HDA/afm\"\n",
    "a = indexGen(use_len= 5, out_len = 16, rand= True)\n",
    "for i in os.listdir(path):\n",
    "    if \"HDA_5\" in i:\n",
    "        print(a.get(f\"{path}/{i}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "epoch = 0\n",
    "pbar = tqdm(total = 100,desc = str(epoch), position = 0)\n",
    "for i in range(10):\n",
    "    pbar.update(1)\n",
    "    epoch += 1\n",
    "pbar.close()\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from network.NLayerNN import NLayerDiscriminator\n",
    "netA = NLayerDiscriminator(in_channels = 1)\n",
    "netB = NLayerDiscriminator(in_channels = 1)\n",
    "netA.requires_grad_(False)\n",
    "netB.requires_grad_(True)\n",
    "inp = torch.rand((1,1,128,128))\n",
    "x = netA(inp)\n",
    "x = netB(inp * x)\n",
    "netA.requires_grad_(True)\n",
    "netB.requires_grad_(False)\n",
    "x = netA(inp)\n",
    "x = netB(inp * x)\n",
    "x.backward()\n",
    "netB.requires_grad_(True)\n",
    "\n",
    "\n",
    "grads = []\n",
    "for param in netA.parameters():\n",
    "    if param.grad is None:\n",
    "        print(\"None\")\n",
    "        break\n",
    "    grads.append(param.grad.view(-1))\n",
    "else:\n",
    "    grads = torch.cat(grads)\n",
    "    print(grads.mean())\n",
    "\n",
    "grads = []\n",
    "for param in netB.parameters():\n",
    "    if param.grad is None:\n",
    "        print(\"None\")\n",
    "        break\n",
    "    grads.append(param.grad.view(-1))\n",
    "else:\n",
    "    grads = torch.cat(grads)\n",
    "    print(grads.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.logger import Logger\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "a = Logger(\"/home/supercgor/gitfile/ARAI\", \"123.log\")\n",
    "top = tqdm(total= 100, desc=f\"TOP\", position=0, leave=True, unit='it')\n",
    "mid = tqdm(total= 50, desc=f\"MID\", position=1, leave=False, unit='it')\n",
    "bot = tqdm(total= 50, desc=f\"BOT\", position=2, leave=False, unit='it')\n",
    "for i in range(100):\n",
    "    for j in range(50):\n",
    "        time.sleep(0.2)\n",
    "        mid.update(1)\n",
    "    for j in range(50):\n",
    "        time.sleep(0.1)\n",
    "        bot.update(1)        \n",
    "    top.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(10000,2)\n",
    "b = np.random.rand(10, 10000) > 0.5\n",
    "b @ a / 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.unet3d_model import TransUNet3D\n",
    "from network.basic import basicParallel\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "preds.append(torch.load(\"/home/supercgor/gitfile/ARAI/datasets/data/exp/npy/9A_basel/HDA_3_-1_-1_1_1.npy\"))\n",
    "preds.append(torch.load(\"/home/supercgor/gitfile/ARAI/datasets/data/exp/npy/9A_basel/HDA_3_-1_-1_0_1.npy\"))\n",
    "preds = np.asarray(preds)\n",
    "preds = torch.from_numpy(preds)\n",
    "sp = tuple(preds.shape)\n",
    "sp = (*sp[:-1], sp[-1] // 4 , 4)\n",
    "preds = preds.reshape(sp)\n",
    "print(preds.shape)\n",
    "preds = torch.permute(preds, (0, 4, 1, 2, 3, 5))\n",
    "a = np.moveaxis(np.indices((32,32,9)), 0, -1)\n",
    "print(preds.shape)\n",
    "pos = (preds[...,:3] + a)\n",
    "pos = pos.reshape((*pos.shape[:2], -1, 3)) \n",
    "select = (preds[...,3] > 0).reshape(*pos.shape[:2], -1)\n",
    "print(pos.shape)\n",
    "print(select.shape)\n",
    "pos = pos.mul(select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "class a():\n",
    "    def __init__(self, b: OrderedDict):\n",
    "        self.b = b\n",
    "\n",
    "e = {\"O\": 1.4, \"H\": 1.5}\n",
    "c = OrderedDict(e)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "import math\n",
    "_ = torch.manual_seed(123)\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "fid = FrechetInceptionDistance(feature=64).cuda()\n",
    "imgs_dist1 = torch.ones((20, 4, 32, 32), device=\"cuda\") * 255\n",
    "imgs_dist1 = imgs_dist1.to(dtype = torch.uint8)\n",
    "row = math.ceil(math.sqrt(imgs_dist1.shape[1]))\n",
    "imgs_dist1 = imgs_dist1.view(20,4, 1, 32, 32)\n",
    "imgs_dist1 = imgs_dist1.expand(-1, -1, 3, -1, -1)\n",
    "\n",
    "imgs_dist2 = torch.randn((20, 4, 32, 32), device=\"cuda\") * 255\n",
    "imgs_dist2 = imgs_dist2.to(dtype = torch.uint8)\n",
    "row = math.ceil(math.sqrt(imgs_dist2.shape[1]))\n",
    "imgs_dist2 = imgs_dist2.view(20,4, 1, 32, 32)\n",
    "imgs_dist2 = imgs_dist2.expand(-1, -1, 3, -1, -1)\n",
    "\n",
    "for i in imgs_dist1:\n",
    "    img = make_grid(i, nrow = row, padding= 0)\n",
    "    fid.update(i, real=True)\n",
    "\n",
    "for i in imgs_dist2:\n",
    "    img = make_grid(i, nrow = row, padding= 0)\n",
    "    fid.update(i, real=False)\n",
    "\n",
    "fid.compute()\n",
    "\n",
    "for i in imgs_dist1:\n",
    "    img = make_grid(i, nrow = row, padding= 0)\n",
    "    fid.update(i, real=True)\n",
    "\n",
    "for i in imgs_dist2:\n",
    "    img = make_grid(i, nrow = row, padding= 0)\n",
    "    fid.update(i, real=False)\n",
    "\n",
    "\n",
    "fid.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "predictions = torch.randn((16, 32, 32, 4, 8))\n",
    "batch, X, Y, Z, last = predictions.shape\n",
    "P = predictions[..., range(3,last,4)] > 0\n",
    "P = torch.permute(P, (0, 4, 3, 1, 2))  # X, Y, Z, C -> C, Z, H, W\n",
    "P = P.reshape((batch * 2 * Z, 1, X, Y))\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset import read_pic\n",
    "from torchvision.utils import save_image\n",
    "imgs = read_pic(\"/home/supercgor/gitfile/ARAI/datasets/data/exp/afm/HDA_3_-1_-1_0_0\", [0,1,2,3])\n",
    "print(imgs.shape)\n",
    "imgs = imgs.view(1, 256, 256)\n",
    "save_image(imgs, \"./img.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.analyze_data import Analyzer, Analyzer2\n",
    "from utils.loader import poscarLoader\n",
    "from config import get_config\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torch\n",
    "a = torch.load(\"/home/supercgor/gitfile/ARAI/datasets/data/exp/npy/3A_with_more_data/HDA_3_-1_-1_0_1.npy\")\n",
    "b = torch.load(\"/home/supercgor/gitfile/ARAI/datasets/data/exp/npy/3A_with_more_data/HDA_3_-1_-1_0_1.npy\")\n",
    "a, b = torch.from_numpy(a), torch.from_numpy(b)\n",
    "B = Analyzer2(sort=True, threshold = 0)\n",
    "C = Analyzer(get_config())\n",
    "info = E = C(a,b)\n",
    "F = B(a,b)\n",
    "#print(OP)\n",
    "print(len(E['P_nms'][0][0]), len(set(E['TP_index_nms'][0][0][0].tolist())), len(E['P_nms'][0][1]), len(set(E['TP_index_nms'][0][1][0].tolist())))\n",
    "print(len(B.TP['O'][0]), len(B.FP['O'][0]), len(B.FN['O'][0]), len(B.TP['H'][0]), len(B.FP['H'][0]), len(B.FN['H'][0]))\n",
    "poscarLoader.pos2poscar(\"./test.poscar\",OrderedDict(O = B.P['O'][0], H = B.P['H'][0]))\n",
    "print(f\"{len(B.TP['O'][0])}\\n\",B.TP['O'][0], f\"\\n{len(B.TP['H'][0])}\\n\",B.TP[\"H\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = poscarLoader.pos2box({\"O\": B.P['O'][0], \"H\": B.P[\"H\"][0]}, (3, 25, 25), (4, 32, 32))\n",
    "IND = box[...,3].nonzero()\n",
    "print(IND.shape)\n",
    "print(box[IND[...,0],IND[...,1],IND[...,2],IND[...,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.stylegan2_pytorch.stylegan2_pytorch import Generator\n",
    "from network.basic import basicModel\n",
    "import torch\n",
    "class g(basicModel, Generator):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        Generator.__init__(self, *args, **kwargs)\n",
    "        basicModel.__init__(self)\n",
    "\n",
    "a = Generator(32,32)\n",
    "b = torch.rand(1,256)\n",
    "c = torch.randn(1,32,32,1)\n",
    "a(b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kornia.filters import filter3d\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import einops    \n",
    "\n",
    "w1 = torch.ones(3, 10, 3, 3, 3)\n",
    "w2 = torch.rand(1, 10)\n",
    "einops.einsum(w1,w2,\"O S P Q R, B S -> (B O) S P Q R\")\n",
    "a = einsum(\"O I P Q R, B S -> B O IS P Q R\", w1, w2)\n",
    "a[0,0]\n",
    "#einops.rearrange(img, \"b c 1 h w -> b c h w\").shape\n",
    "# f = torch.Tensor([1, 2, 1])\n",
    "# f = einsum(\"i,j,k->ijk\", f,f,f)\n",
    "# print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.styleGAN_cond_q3d.op import MapStyle3d\n",
    "from network.basic import structure\n",
    "import torch\n",
    "\n",
    "X = torch.randn(4, 32, 16, 128, 128)\n",
    "Y = torch.randn(4, 32, 16, 64, 64)\n",
    "Z = torch.randn(4, 64, 16, 32, 32)\n",
    "R = torch.randn(4, 128, 8, 16, 16)\n",
    "S = torch.randn(4, 256, 4, 8, 8)\n",
    "\n",
    "a = MapStyle3d(512)\n",
    "a(X)\n",
    "a(Y)\n",
    "a(Z)\n",
    "a(R)\n",
    "a(S)\n",
    "structure(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from network.unet3d_model import UNet3D\n",
    "a = UNet3D()\n",
    "x = torch.randn(4, 1, 16, 128, 128)\n",
    "x = x[:,:,0,0,0]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在target 處的 confidence + 上 0.1的sigma 再放進去discri\n",
    "import torch\n",
    "from network.styleGAN_cond_q3d import StyleGAN3D, Discriminator3D\n",
    "from network.fea_Unet3d import UNet3D\n",
    "from network.basic import structure\n",
    "a = StyleGAN3D(network_capacity = 32).cuda()\n",
    "b = UNet3D().cuda()\n",
    "c = Discriminator3D(network_capacity= 16).cuda()\n",
    "b.requires_grad_(False)\n",
    "# a.requires_grad_(False)\n",
    "x = torch.randn((2, 1, 16, 128, 128)).cuda()\n",
    "x, f = b(x)\n",
    "noise = torch.randn((2, 16, 128, 128, 1)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "result = a(f, noise)\n",
    "result = torch.cat([x, result], dim = 1)\n",
    "print(result.shape)\n",
    "c(result).mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*map(lambda x: x.shape, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "a = torch.randn(2,16)\n",
    "b = torch.randn(512,16)\n",
    "F.linear(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import einsum\n",
    "k = torch.Tensor([1,2,1])\n",
    "einsum(\"i, j, k -> ijk\", k,k,k)[None,...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randn(1, 8, 1, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import repeat, rearrange, einsum\n",
    "import torch\n",
    "POS = torch.FloatTensor(10000,3).uniform_(0,25)\n",
    "x = torch.randn(1, 8 ,25,25,25)\n",
    "s = torch.tensor([0.5,0.5,0.5, 1])\n",
    "y = torch.ones(1, 25,25,25).nonzero().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = rearrange(x, \"B (C E) Z X Y -> B E C (Z X Y)\", C = 4) + y\n",
    "r = torch.einsum(\"B E C R, C -> B E R C\", r, s)\n",
    "a = r[0,0,...]\n",
    "a = a[(a[...,0] > 0.5),:]\n",
    "a = a[a[..., 0].argsort(descending=True),:]\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import metStat\n",
    "split = (0.,3.,6.)\n",
    "met = {(low,up):{f\"{key}\": metStat(mode = m) \n",
    "                     for key,m in [(\"ACC\", \"mean\"), (\"SUC\", \"mean\"), (\"TP\", \"sum\"), (\"FP\", \"sum\"), (\"FN\", \"sum\")] \n",
    "                     } for low,up in zip(split[:-1], split[1:])}\n",
    "print(met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from config import get_config\n",
    "from datasets.dataset import make_dataset\n",
    "torch.set_printoptions(precision=4,sci_mode=False)\n",
    "cfg = get_config()\n",
    "train_loader = make_dataset('train', cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "a = time()\n",
    "r = iter(train_loader)\n",
    "print(len(train_loader))\n",
    "for i in range(len(train_loader)):\n",
    "    s = next(r)\n",
    "    print(s[0].shape[0])\n",
    "print((time()- a)/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(10)\n",
    "a.cuda(non_blocking=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in zip([1,2],[1]):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m      9\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 11\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/mlenv/lib/python3.10/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/mlenv/lib/python3.10/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/apps/miniconda3/envs/mlenv/lib/python3.10/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from einops import repeat\n",
    "a = torch.zeros(2, 8, 4,10, 10)\n",
    "B, C, Z, X, Y = a.shape\n",
    "b = repeat(torch.rand((2,)), \"B -> B C Z X Y\", C = C, Z = Z, X = X, Y = Y)\n",
    "class a(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x = 1\n",
    "\n",
    "a = torch.load(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  3,  3,  3],\n",
      "        [ 0,  0,  0,  ..., 31, 31, 31],\n",
      "        [ 0,  1,  2,  ..., 29, 30, 31]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "IND = torch.ones(1, *(4, 32, 32)).nonzero().T\n",
    "print(IND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2010, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from utils.criterion import basicLoss\n",
    "from torch import nn\n",
    "\n",
    "r = basicLoss()\n",
    "a = torch.randn(2, 1, 4, 32, 32, 4).clip(0,1)\n",
    "b = torch.randn(2, 1, 4, 32, 32, 4).clip(0,1)\n",
    "m = b[...,0] > 0.7\n",
    "b[m].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
