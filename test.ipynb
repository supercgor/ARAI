{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "offset = 1\n",
    "select_num = 0\n",
    "upper = 4\n",
    "a = list(range(10))\n",
    "r = sample(range(upper), k=(select_num % upper))\n",
    "[i + offset for i in range(upper) for _ in range(select_num // upper + (i in r))]\n",
    "a[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def weighted_split(nums, ratio, mode = math.ceil):\n",
    "        if len(ratio) <= 1:\n",
    "            return nums,\n",
    "        else:\n",
    "            out = mode(nums * ratio[0]/sum(ratio))\n",
    "            return out, *weighted_split(nums - out, ratio = ratio[1:], mode = mode)\n",
    "\n",
    "weighted_split(100, (0.33,0.42,0.17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 999\n",
    "p = (0.3,0.3,0.3)\n",
    "print([a * i / sum(p) for i in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "a.extend(i for i in range(10))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset import indexGen\n",
    "import os\n",
    "path = \"/home/supercgor/gitfile/ARAI/datasets/data/HDA/afm\"\n",
    "a = indexGen(use_len= 5, out_len = 16, rand= True)\n",
    "for i in os.listdir(path):\n",
    "    if \"HDA_5\" in i:\n",
    "        print(a.get(f\"{path}/{i}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "epoch = 0\n",
    "pbar = tqdm(total = 100,desc = str(epoch), position = 0)\n",
    "for i in range(10):\n",
    "    pbar.update(1)\n",
    "    epoch += 1\n",
    "pbar.close()\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from network.NLayerNN import NLayerDiscriminator\n",
    "netA = NLayerDiscriminator(in_channels = 1)\n",
    "netB = NLayerDiscriminator(in_channels = 1)\n",
    "netA.requires_grad_(False)\n",
    "netB.requires_grad_(True)\n",
    "inp = torch.rand((1,1,128,128))\n",
    "x = netA(inp)\n",
    "x = netB(inp * x)\n",
    "netA.requires_grad_(True)\n",
    "netB.requires_grad_(False)\n",
    "x = netA(inp)\n",
    "x = netB(inp * x)\n",
    "x.backward()\n",
    "netB.requires_grad_(True)\n",
    "\n",
    "\n",
    "grads = []\n",
    "for param in netA.parameters():\n",
    "    if param.grad is None:\n",
    "        print(\"None\")\n",
    "        break\n",
    "    grads.append(param.grad.view(-1))\n",
    "else:\n",
    "    grads = torch.cat(grads)\n",
    "    print(grads.mean())\n",
    "\n",
    "grads = []\n",
    "for param in netB.parameters():\n",
    "    if param.grad is None:\n",
    "        print(\"None\")\n",
    "        break\n",
    "    grads.append(param.grad.view(-1))\n",
    "else:\n",
    "    grads = torch.cat(grads)\n",
    "    print(grads.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.logger import Logger\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "a = Logger(\"/home/supercgor/gitfile/ARAI\", \"123.log\")\n",
    "top = tqdm(total= 100, desc=f\"TOP\", position=0, leave=True, unit='it')\n",
    "mid = tqdm(total= 50, desc=f\"MID\", position=1, leave=False, unit='it')\n",
    "bot = tqdm(total= 50, desc=f\"BOT\", position=2, leave=False, unit='it')\n",
    "for i in range(100):\n",
    "    for j in range(50):\n",
    "        time.sleep(0.2)\n",
    "        mid.update(1)\n",
    "    for j in range(50):\n",
    "        time.sleep(0.1)\n",
    "        bot.update(1)        \n",
    "    top.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(10000,2)\n",
    "b = np.random.rand(10, 10000) > 0.5\n",
    "b @ a / 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.unet3d_model import TransUNet3D\n",
    "from network.basic import basicParallel\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "preds.append(torch.load(\"/home/supercgor/gitfile/ARAI/datasets/data/exp/npy/9A_basel/HDA_3_-1_-1_1_1.npy\"))\n",
    "preds.append(torch.load(\"/home/supercgor/gitfile/ARAI/datasets/data/exp/npy/9A_basel/HDA_3_-1_-1_0_1.npy\"))\n",
    "preds = np.asarray(preds)\n",
    "preds = torch.from_numpy(preds)\n",
    "sp = tuple(preds.shape)\n",
    "sp = (*sp[:-1], sp[-1] // 4 , 4)\n",
    "preds = preds.reshape(sp)\n",
    "print(preds.shape)\n",
    "preds = torch.permute(preds, (0, 4, 1, 2, 3, 5))\n",
    "a = np.moveaxis(np.indices((32,32,9)), 0, -1)\n",
    "print(preds.shape)\n",
    "pos = (preds[...,:3] + a)\n",
    "pos = pos.reshape((*pos.shape[:2], -1, 3)) \n",
    "select = (preds[...,3] > 0).reshape(*pos.shape[:2], -1)\n",
    "print(pos.shape)\n",
    "print(select.shape)\n",
    "pos = pos.mul(select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "class a():\n",
    "    def __init__(self, b: OrderedDict):\n",
    "        self.b = b\n",
    "\n",
    "e = {\"O\": 1.4, \"H\": 1.5}\n",
    "c = OrderedDict(e)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "import math\n",
    "_ = torch.manual_seed(123)\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "fid = FrechetInceptionDistance(feature=64).cuda()\n",
    "imgs_dist1 = torch.ones((20, 4, 32, 32), device=\"cuda\") * 255\n",
    "imgs_dist1 = imgs_dist1.to(dtype = torch.uint8)\n",
    "row = math.ceil(math.sqrt(imgs_dist1.shape[1]))\n",
    "imgs_dist1 = imgs_dist1.view(20,4, 1, 32, 32)\n",
    "imgs_dist1 = imgs_dist1.expand(-1, -1, 3, -1, -1)\n",
    "\n",
    "imgs_dist2 = torch.randn((20, 4, 32, 32), device=\"cuda\") * 255\n",
    "imgs_dist2 = imgs_dist2.to(dtype = torch.uint8)\n",
    "row = math.ceil(math.sqrt(imgs_dist2.shape[1]))\n",
    "imgs_dist2 = imgs_dist2.view(20,4, 1, 32, 32)\n",
    "imgs_dist2 = imgs_dist2.expand(-1, -1, 3, -1, -1)\n",
    "\n",
    "for i in imgs_dist1:\n",
    "    img = make_grid(i, nrow = row, padding= 0)\n",
    "    fid.update(i, real=True)\n",
    "\n",
    "for i in imgs_dist2:\n",
    "    img = make_grid(i, nrow = row, padding= 0)\n",
    "    fid.update(i, real=False)\n",
    "\n",
    "fid.compute()\n",
    "\n",
    "for i in imgs_dist1:\n",
    "    img = make_grid(i, nrow = row, padding= 0)\n",
    "    fid.update(i, real=True)\n",
    "\n",
    "for i in imgs_dist2:\n",
    "    img = make_grid(i, nrow = row, padding= 0)\n",
    "    fid.update(i, real=False)\n",
    "\n",
    "\n",
    "fid.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "predictions = torch.randn((16, 32, 32, 4, 8))\n",
    "batch, X, Y, Z, last = predictions.shape\n",
    "P = predictions[..., range(3,last,4)] > 0\n",
    "P = torch.permute(P, (0, 4, 3, 1, 2))  # X, Y, Z, C -> C, Z, H, W\n",
    "P = P.reshape((batch * 2 * Z, 1, X, Y))\n",
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset import read_pic\n",
    "from torchvision.utils import save_image\n",
    "imgs = read_pic(\"/home/supercgor/gitfile/ARAI/datasets/data/exp/afm/HDA_3_-1_-1_0_0\", [0,1,2,3])\n",
    "print(imgs.shape)\n",
    "imgs = imgs.view(1, 256, 256)\n",
    "save_image(imgs, \"./img.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import Analyzer, Analyzer2\n",
    "from utils.loader import poscarLoader\n",
    "from config import get_config\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torch\n",
    "a = torch.load(\"/home/supercgor/gitfile/ARAI/datasets/data/exp/npy/3A_with_more_data/HDA_3_-1_-1_0_1.npy\")\n",
    "b = torch.load(\"/home/supercgor/gitfile/ARAI/datasets/data/exp/npy/3A_with_more_data/HDA_3_-1_-1_0_1.npy\")\n",
    "a, b = torch.from_numpy(a), torch.from_numpy(b)\n",
    "B = Analyzer2(sort=True, threshold = 0)\n",
    "C = Analyzer(get_config())\n",
    "info = E = C(a,b)\n",
    "F = B(a,b)\n",
    "#print(OP)\n",
    "print(len(E['P_nms'][0][0]), len(set(E['TP_index_nms'][0][0][0].tolist())), len(E['P_nms'][0][1]), len(set(E['TP_index_nms'][0][1][0].tolist())))\n",
    "print(len(B.TP['O'][0]), len(B.FP['O'][0]), len(B.FN['O'][0]), len(B.TP['H'][0]), len(B.FP['H'][0]), len(B.FN['H'][0]))\n",
    "poscarLoader.pos2poscar(\"./test.poscar\",OrderedDict(O = B.P['O'][0], H = B.P['H'][0]))\n",
    "print(f\"{len(B.TP['O'][0])}\\n\",B.TP['O'][0], f\"\\n{len(B.TP['H'][0])}\\n\",B.TP[\"H\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = poscarLoader.pos2box({\"O\": B.P['O'][0], \"H\": B.P[\"H\"][0]}, (3, 25, 25), (4, 32, 32))\n",
    "IND = box[...,3].nonzero()\n",
    "print(IND.shape)\n",
    "print(box[IND[...,0],IND[...,1],IND[...,2],IND[...,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.stylegan2_pytorch.stylegan2_pytorch import Generator\n",
    "from network.basic import basicModel\n",
    "import torch\n",
    "class g(basicModel, Generator):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        Generator.__init__(self, *args, **kwargs)\n",
    "        basicModel.__init__(self)\n",
    "\n",
    "a = Generator(32,32)\n",
    "b = torch.rand(1,256)\n",
    "c = torch.randn(1,32,32,1)\n",
    "a(b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kornia.filters import filter3d\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import einops    \n",
    "\n",
    "w1 = torch.ones(3, 10, 3, 3, 3)\n",
    "w2 = torch.rand(1, 10)\n",
    "einops.einsum(w1,w2,\"O S P Q R, B S -> (B O) S P Q R\")\n",
    "a = einsum(\"O I P Q R, B S -> B O IS P Q R\", w1, w2)\n",
    "a[0,0]\n",
    "#einops.rearrange(img, \"b c 1 h w -> b c h w\").shape\n",
    "# f = torch.Tensor([1, 2, 1])\n",
    "# f = einsum(\"i,j,k->ijk\", f,f,f)\n",
    "# print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.styleGAN_cond_q3d.op import MapStyle3d\n",
    "from network.basic import structure\n",
    "import torch\n",
    "\n",
    "X = torch.randn(4, 32, 16, 128, 128)\n",
    "Y = torch.randn(4, 32, 16, 64, 64)\n",
    "Z = torch.randn(4, 64, 16, 32, 32)\n",
    "R = torch.randn(4, 128, 8, 16, 16)\n",
    "S = torch.randn(4, 256, 4, 8, 8)\n",
    "\n",
    "a = MapStyle3d(512)\n",
    "a(X)\n",
    "a(Y)\n",
    "a(Z)\n",
    "a(R)\n",
    "a(S)\n",
    "structure(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from network.unet3d_model import UNet3D\n",
    "a = UNet3D()\n",
    "x = torch.randn(4, 1, 16, 128, 128)\n",
    "x = x[:,:,0,0,0]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在target 處的 confidence + 上 0.1的sigma 再放進去discri\n",
    "import torch\n",
    "from network.styleGAN_cond_q3d import StyleGAN3D, Discriminator3D\n",
    "from network.fea_Unet3d import UNet3D\n",
    "from network.basic import structure\n",
    "a = StyleGAN3D(network_capacity = 32).cuda()\n",
    "b = UNet3D().cuda()\n",
    "c = Discriminator3D(network_capacity= 16).cuda()\n",
    "b.requires_grad_(False)\n",
    "# a.requires_grad_(False)\n",
    "x = torch.randn((2, 1, 16, 128, 128)).cuda()\n",
    "x, f = b(x)\n",
    "noise = torch.randn((2, 16, 128, 128, 1)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "result = a(f, noise)\n",
    "result = torch.cat([x, result], dim = 1)\n",
    "print(result.shape)\n",
    "c(result).mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*map(lambda x: x.shape, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "a = torch.randn(2,16)\n",
    "b = torch.randn(512,16)\n",
    "F.linear(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import einsum\n",
    "k = torch.Tensor([1,2,1])\n",
    "einsum(\"i, j, k -> ijk\", k,k,k)[None,...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randn(1, 8, 1, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import repeat, rearrange, einsum\n",
    "import torch\n",
    "POS = torch.FloatTensor(10000,3).uniform_(0,25)\n",
    "x = torch.randn(1, 8 ,25,25,25)\n",
    "s = torch.tensor([0.5,0.5,0.5, 1])\n",
    "y = torch.ones(1, 25,25,25).nonzero().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = rearrange(x, \"B (C E) Z X Y -> B E C (Z X Y)\", C = 4) + y\n",
    "r = torch.einsum(\"B E C R, C -> B E R C\", r, s)\n",
    "a = r[0,0,...]\n",
    "a = a[(a[...,0] > 0.5),:]\n",
    "a = a[a[..., 0].argsort(descending=True),:]\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import metStat\n",
    "split = (0.,3.,6.)\n",
    "met = {(low,up):{f\"{key}\": metStat(mode = m) \n",
    "                     for key,m in [(\"ACC\", \"mean\"), (\"SUC\", \"mean\"), (\"TP\", \"sum\"), (\"FP\", \"sum\"), (\"FN\", \"sum\")] \n",
    "                     } for low,up in zip(split[:-1], split[1:])}\n",
    "print(met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from config import get_config\n",
    "from datasets.dataset import make_dataset\n",
    "torch.set_printoptions(precision=4,sci_mode=False)\n",
    "cfg = get_config()\n",
    "train_loader = make_dataset('train', cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "a = time()\n",
    "r = iter(train_loader)\n",
    "print(len(train_loader))\n",
    "for i in range(len(train_loader)):\n",
    "    s = next(r)\n",
    "    print(s[0].shape[0])\n",
    "print((time()- a)/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(10)\n",
    "a.cuda(non_blocking=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in zip([1,2],[1]):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import repeat\n",
    "a = torch.zeros(2, 8, 4,10, 10)\n",
    "B, C, Z, X, Y = a.shape\n",
    "b = repeat(torch.rand((2,)), \"B -> B C Z X Y\", C = C, Z = Z, X = X, Y = Y)\n",
    "class a(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x = 1\n",
    "\n",
    "a = torch.load(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "IND = torch.ones(1, *(4, 32, 32)).nonzero().T\n",
    "print(IND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.criterion import focalLoss,basicLoss\n",
    "from torch import nn\n",
    "from torch.nn import BCELoss\n",
    "l = focalLoss()\n",
    "c = BCELoss()\n",
    "bb = basicLoss()\n",
    "a = torch.randn(2, 4, 32, 32, 4).clip(0,1)\n",
    "b = torch.randn(2, 4, 32, 32, 4).clip(0,1)\n",
    "print(l(a,b), c(a,b),bb(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.criterion import focalLoss,basicLoss\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "a = torch.randn(1, 1, 3, 3, 3)\n",
    "b = nn.Sigmoid()\n",
    "c = b(a)\n",
    "print(a,\"\\n\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randn(10,50)\n",
    "i = a.min(1).indices\n",
    "b = torch.full((50,), False)\n",
    "b[i] = True\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.criterion import focalLoss,basicLoss\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "a = torch.randn(100, 3)\n",
    "mask = a[...,0] > 0\n",
    "a[mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from utils.criterion import focalLoss\n",
    "import torch\n",
    "a = torch.tensor([[[0,0,0],[0,1.,0],[0,0,0]],[[0,0,0],[1,0,1],[0,0,0]]])\n",
    "c = torch.randn(2,3,3)\n",
    "pw = torch.tensor((5.0,5.0,5.0))\n",
    "l1 = nn.BCEWithLogitsLoss(pos_weight=pw)\n",
    "l2 = nn.BCELoss()\n",
    "l3 = focalLoss(alpha = 0.9)\n",
    "e = l1(c,a)\n",
    "c = c.sigmoid()\n",
    "print(l2(c,a)/e)\n",
    "print(15 * l3(c,a)/e)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset import make_dataset\n",
    "from utils.criterion import basicLoss\n",
    "from config import get_config\n",
    "cfg = get_config()\n",
    "loader = make_dataset('train', cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "inputs, tar, _ = next(a)\n",
    "print(tar.shape)\n",
    "tar = rearrange(tar[0], \"(E C) Z X Y -> Z X Y E C\", C = 4)\n",
    "IND = tar[...,0] > 0.5\n",
    "print(tar[IND])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "test = torch.tensor([1,2,3,4,5,6,7,8])\n",
    "rearrange(test, \"(C E) -> E C\", C = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class blockDict():\n",
    "    def __init__(self, *args, style = \"default\", **kwargs):\n",
    "        if style == \"default\":\n",
    "            D = defaultdict\n",
    "        else:\n",
    "            D = dict\n",
    "        self._dict = D(args)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "defaultdict(None,[(\"abc\",),(\"ab\",)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import time\n",
    "pred = torch.rand(50,3)\n",
    "targ = torch.rand(100,3)\n",
    "distance = 0.1\n",
    "t = time.time()\n",
    "for _ in range(1):\n",
    "    DIS_MAT = torch.cdist(pred, targ) < distance\n",
    "    SELECT_P = DIS_MAT.sum(1) != 0\n",
    "    SELECT_T = torch.full((targ.shape[0], ), False)\n",
    "    mask = DIS_MAT[SELECT_P]\n",
    "    if mask.nelement() != 0:\n",
    "        mask = mask.min(1).indices \n",
    "        SELECT_T[mask] = True\n",
    "\n",
    "print(SELECT_P)\n",
    "print(time.time() - t)\n",
    "t = time.time()\n",
    "\n",
    "for _ in range(1): \n",
    "    DIS_MAT = torch.cdist(pred, targ)\n",
    "    SELECT_T = DIS_MAT.sum(0) != 0\n",
    "    SELECT_P = torch.full((targ.shape[0], ), False)\n",
    "    mask = DIS_MAT[SELECT_P]\n",
    "    if mask.nelement() != 0:\n",
    "        mask = mask.min(1).indices \n",
    "        SELECT_T[mask] = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(((2,1,3),(4,4,5),(1,2,3),(2,1,3)))\n",
    "VI, TI = torch.min(a, dim = 1)\n",
    "mask = VI < 4\n",
    "PI = TI[mask]\n",
    "V = VI[mask]\n",
    "T = torch.full((a.shape[1],), False)\n",
    "T[PI] = True\n",
    "print(T, V)\n",
    "\n",
    "a = torch.tensor(((1,2,3),(4,4,5),(1,2,3)))\n",
    "V, TI = torch.min(a, dim = 1)\n",
    "PI = TI[V < 4]\n",
    "a[PI]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets.dataset import indexGen\n",
    "import random\n",
    "import torchvision.transforms as tf\n",
    "from torch import nn\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import math\n",
    "a = torch.load(\"/home/supercgor/gitfile/ARAI/datasets/data/bulkice/datapack/T160_1.npz\")\n",
    "r = indexGen()\n",
    "Is = a['imgs'].shape[0]\n",
    "IMGS = a['imgs'][r.get(Is)]\n",
    "\n",
    "class PixelShift(nn.Module):\n",
    "    def __init__(self, max_shift = (5,5), fill = ..., ref = 5):\n",
    "        super().__init__()\n",
    "        self.max_shift = max_shift\n",
    "        self.fill = fill\n",
    "        self.ref = ref\n",
    "\n",
    "    def forward(self, x): # shape (B ,C, X, Y)\n",
    "        for i in range(0, x.shape[0]):\n",
    "            if i == self.ref:\n",
    "                continue\n",
    "            shift = (random.randint(-self.max_shift[0], self.max_shift[0]), random.randint(-self.max_shift[1], self.max_shift[1]))\n",
    "            x[i] = torch.roll(x[i], shift, (1,2))\n",
    "            if self.fill is not None:\n",
    "                fill = self._fill(x[i])\n",
    "                if shift[0] > 0:\n",
    "                    x[i,:,:shift[0],:] = fill\n",
    "                elif shift[0] < 0:\n",
    "                    x[i,:,shift[0]:,:] = fill\n",
    "                if shift[1] > 0:\n",
    "                    x[i,:,:,:shift[1]] = fill\n",
    "                elif shift[1] < 0:\n",
    "                    x[i,:,:,shift[1]:] = fill\n",
    "        return x\n",
    "    \n",
    "    def _fill(self, y):\n",
    "        if self.fill is ...:\n",
    "            return 0\n",
    "        elif self.fill == \"mean\":\n",
    "            return y.mean().item()\n",
    "\n",
    "class CutOut(nn.Module):\n",
    "    def __init__(self, max_n = 4, scale = (0.1,0.9), ratio = 0.02):\n",
    "        super().__init__()\n",
    "        self.max_n = max_n\n",
    "        self.ratio = ratio\n",
    "        self.scale = scale\n",
    "    def forward(self, x):\n",
    "        B, C, X, Y = x.shape\n",
    "        area = X * Y * 0.1\n",
    "        for i in range(B):\n",
    "            for _ in range(random.randint(0, self.max_n)):\n",
    "                use_area = random.randint(int(area * 0.1), int(area))\n",
    "                pos = (random.randint(0, X), random.randint(0, Y))\n",
    "                if random.randbytes(1):\n",
    "                    rd_x = random.randint(int(X * self.scale[0]), int(X * self.scale[1]))\n",
    "                    rd_y = use_area // rd_x\n",
    "                else:\n",
    "                    rd_y = random.randin(int(Y * self.scale[0]), int(Y * self.scale[1]))\n",
    "                    rd_x = use_area // rd_y\n",
    "                value = x[i].mean() * random.uniform(0.5, 1.5)\n",
    "                value.clip_(0, 1)\n",
    "                x[i, :, pos[0]-rd_x//2: pos[0]+rd_x//2, pos[1] - rd_y//2:pos[1]+rd_y//2] = value\n",
    "        return x\n",
    "    \n",
    "class Noisy(nn.Module):\n",
    "    def __init__(self, intensity = 0.1, mode = [\"add\", \"times\", None], noisy = [\"uniform\", \"normal\"], add_factor = 1):\n",
    "        super().__init__()\n",
    "        self.int = intensity\n",
    "        self.noisy = noisy\n",
    "        self.mode = mode\n",
    "        self.add_factor = add_factor\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(x.shape[0]):\n",
    "            noisy, mode = random.choice(self.noisy), random.choice(self.mode)\n",
    "            if mode is None:\n",
    "                continue\n",
    "            else:\n",
    "                noise = torch.FloatTensor(x.shape[1:])\n",
    "                if  noisy == \"normal\":\n",
    "                    noise.normal_(0, self.int)\n",
    "                elif noisy == \"uniform\":\n",
    "                    noise.uniform_(-self.int, self.int)\n",
    "                if mode == \"add\":\n",
    "                    x[i].add_(noise * self.add_factor)\n",
    "                elif mode == \"times\":\n",
    "                    x[i].mul_(1 + noise)\n",
    "        x.clip_(0 , 1)\n",
    "        return x\n",
    "\n",
    "class ColorJitter(nn.Module):\n",
    "    def __init__(self, B = 0.3, C = 0.3, S = 0.0):\n",
    "        super().__init__()\n",
    "        self.J = tf.ColorJitter(brightness = B, contrast = C, saturation = S)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(x.shape[0]):\n",
    "            x[i] = self.J(x[i])\n",
    "        return x\n",
    "\n",
    "trans = torch.nn.Sequential(\n",
    "    PixelShift(fill = None),\n",
    "    CutOut(),\n",
    "    Noisy(),\n",
    "    ColorJitter()\n",
    ")\n",
    "\n",
    "trans(IMGS)\n",
    "save_image(make_grid(IMGS),\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid, save_image\n",
    "from datasets.dataset import AFMDataset\n",
    "test = AFMDataset(\"/home/supercgor/gitfile/ARAI/datasets/data/bulkice\", preload=False)\n",
    "save_image(make_grid(test[0][0]), \"test0.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Choose the `slow_r50` model \n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.basic import structure\n",
    "# structure(model)\n",
    "inputs = torch.randn(1, 3, 16, 128, 128)\n",
    "model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(5, 1000).cuda()\n",
    "raw = torch.randn(5, 10).cuda()\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1000, 1000),\n",
    "    nn.Linear(1000, 1000))\n",
    "model2 = nn.Sequential(\n",
    "    nn.Linear(1000, 1000),\n",
    "    nn.Linear(1000, 1000))\n",
    "model3 = nn.Sequential(\n",
    "    nn.Linear(1000, 1000),\n",
    "    nn.Linear(1000, 1000))\n",
    "fea = nn.Sequential(nn.Linear(10,1000))\n",
    "model.requires_grad_(True)\n",
    "model2.requires_grad_(True)\n",
    "model3.requires_grad_(True)\n",
    "fea.requires_grad_(True)\n",
    "model.cuda()\n",
    "model2.cuda()\n",
    "model3.cuda()\n",
    "fea.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "m = []\n",
    "x = inp\n",
    "out = fea(raw)\n",
    "loss = torch.tensor(0.0, requires_grad=True).cuda()\n",
    "m.append(torch.cuda.memory_allocated())\n",
    "\n",
    "1\n",
    "x = x + out\n",
    "x = model(x)\n",
    "loss = x.mean()\n",
    "# loss.backward(retain_graph=True)\n",
    "# loss.zero_()\n",
    "x = x.detach()\n",
    "m.append(torch.cuda.memory_allocated())\n",
    "# 2\n",
    "x = x + out\n",
    "x = model2(x)\n",
    "loss = loss + x.mean()\n",
    "# loss.backward(retain_graph=True)\n",
    "# loss.zero_()\n",
    "x = x.detach()\n",
    "m.append(torch.cuda.memory_allocated())\n",
    "# 3\n",
    "x = x + out\n",
    "x = model3(x)\n",
    "loss = loss + x.mean()\n",
    "loss.backward()\n",
    "x = x.detach()\n",
    "m.append(torch.cuda.memory_allocated())\n",
    "m = [i - m[0] for i in m]\n",
    "print(x)\n",
    "print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numbers\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class GaussianSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply gaussian smoothing on a\n",
    "    1d, 2d or 3d tensor. Filtering is performed seperately for each channel\n",
    "    in the input using a depthwise convolution.\n",
    "    Arguments:\n",
    "        channels (int, sequence): Number of channels of the input tensors. Output will\n",
    "            have this number of channels as well.\n",
    "        kernel_size (int, sequence): Size of the gaussian kernel.\n",
    "        sigma (float, sequence): Standard deviation of the gaussian kernel.\n",
    "        dim (int, optional): The number of dimensions of the data.\n",
    "            Default value is 2 (spatial).\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, kernel_size, sigma, dim=2):\n",
    "        super(GaussianSmoothing, self).__init__()\n",
    "        if isinstance(kernel_size, numbers.Number):\n",
    "            kernel_size = [kernel_size] * dim\n",
    "        if isinstance(sigma, numbers.Number):\n",
    "            sigma = [sigma] * dim\n",
    "\n",
    "        # The gaussian kernel is the product of the\n",
    "        # gaussian function of each dimension.\n",
    "        kernel = 1\n",
    "        meshgrids = torch.meshgrid(\n",
    "            [\n",
    "                torch.arange(size, dtype=torch.float32)\n",
    "                for size in kernel_size\n",
    "            ]\n",
    "        )\n",
    "        for size, std, mgrid in zip(kernel_size, sigma, meshgrids):\n",
    "            mean = (size - 1) / 2\n",
    "            kernel *= 1 / (std * math.sqrt(2 * math.pi)) * \\\n",
    "                      torch.exp(-((mgrid - mean) / (2 * std)) ** 2)\n",
    "\n",
    "        # Make sure sum of values in gaussian kernel equals 1.\n",
    "        kernel = kernel / torch.sum(kernel)\n",
    "\n",
    "        # Reshape to depthwise convolutional weight\n",
    "        kernel = kernel.view(1, 1, *kernel.size())\n",
    "        kernel = kernel.repeat(channels, *[1] * (kernel.dim() - 1))\n",
    "\n",
    "        self.register_buffer('weight', kernel)\n",
    "        self.groups = channels\n",
    "\n",
    "        if dim == 1:\n",
    "            self.conv = F.conv1d\n",
    "        elif dim == 2:\n",
    "            self.conv = F.conv2d\n",
    "        elif dim == 3:\n",
    "            self.conv = F.conv3d\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                'Only 1, 2 and 3 dimensions are supported. Received {}.'.format(dim)\n",
    "            )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Apply gaussian filter to input.\n",
    "        Arguments:\n",
    "            input (torch.Tensor): Input to apply gaussian filter on.\n",
    "        Returns:\n",
    "            filtered (torch.Tensor): Filtered output.\n",
    "        \"\"\"\n",
    "        return self.conv(input, weight=self.weight, groups=self.groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "a = torch.zeros(1, 20, 20)\n",
    "gb = GaussianSmoothing(1, kernel_size = 11, sigma = 1, dim = 2)\n",
    "a[0, 10,10] = 1\n",
    "a = gb(a)\n",
    "a = a/ a.max()\n",
    "save_image(a, \"./gb.png\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from model import *\n",
    "import time\n",
    "# A = VapSR(256, 64, lattent= 64, attn_channels = 80).cuda()\n",
    "# inp = torch.randn(1, 256, 32, 32, 8).cuda()\n",
    "U = UNet3D().cuda().eval()\n",
    "F = FMN().cuda()\n",
    "S = VapSR(in_channels= 256, out_channels= 3).cuda()\n",
    "\n",
    "#U.structure()\n",
    "#F.structure()\n",
    "#S.structure()\n",
    "inp = torch.randn(1, 1, 16, 128, 128).cuda()\n",
    "t = time.time()\n",
    "for _ in range(5): # cuda time: 4.38s / 50\n",
    "    out = U(inp)\n",
    "print(time.time() - t)\n",
    "for key, value in out.items():\n",
    "    print(key, value.shape)\n",
    "t = time.time()\n",
    "for _ in range(5): # cuda time: 4.76s / 50\n",
    "    out2 = F(out)\n",
    "print(time.time() - t)\n",
    "t = time.time()\n",
    "for _ in range(5): # cuda time: 3.22s / 50\n",
    "    out3 = S(out2)\n",
    "print(time.time() - t)\n",
    "print(out3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from model import *\n",
    "from utils.criterion import MultiCLSFocalLoss as MCFL\n",
    "import time\n",
    "net = ARAI().cuda().eval()\n",
    "inp = torch.randn(1, 1, 16, 128, 128).cuda()\n",
    "label = torch.randint(0, 2, (1, 32, 128, 128, 1)).cuda()\n",
    "out = net(inp)\n",
    "CRI = MCFL(pos_weight = (500.0,500.0, 1.0)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CRI(out, label, channels_firtst = False)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randn(10, 5, 5, 5, 3).softmax(-1)\n",
    "b = torch.randint(0, 3, (10, 5, 5, 5, 1))\n",
    "#torch.gather(a, -1, b).shape\n",
    "a = torch.tensor([1.0,3.0,3.0])\n",
    "a[b].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "inp = torch.randn(1, 10, 10, 5, 5)\n",
    "inp.flatten(2).shape\n",
    "\n",
    "def test(a, b):\n",
    "    return b(a)\n",
    "\n",
    "a = - torch.ones(1, 2)\n",
    "b = nn.ReLU(inplace= True)\n",
    "c = a\n",
    "test(a, b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randn(10,10)\n",
    "mask = a > 0\n",
    "ind = (a > 0).nonzero()\n",
    "print(ind)\n",
    "print(a[mask])\n",
    "print(a[ind[:,0], ind[:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import repeat\n",
    "N = 125\n",
    "a = torch.randn(5,5,5).nonzero()\n",
    "c = repeat(a, \"N C -> N M C\", M = N) - repeat(a, \"N C -> M N C\", M = N)\n",
    "c = c.abs()\n",
    "d = torch.randn(5,5,5)\n",
    "print(d)\n",
    "d[c[...,0], c[...,1], c[...,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    i = 2\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {\"O\": 1}\n",
    "b = a\n",
    "a = 0\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "a = OrderedDict()\n",
    "a[\"O\"] = 5\n",
    "a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.mean(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.sparse_coo_tensor(indices= torch.tensor([[0, 1, 1], [0,1,1]]), values= torch.tensor([1,2,3]))\n",
    "a = a[None,...]\n",
    "b = torch.concat([a,a])\n",
    "b == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class a():\n",
    "    def __init__(self):\n",
    "        self.w = 10\n",
    "    \n",
    "    def h(self):\n",
    "        pass\n",
    "    \n",
    "r = a()\n",
    "r.__getattribute__(\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net = CombineModel()\n",
    "imgs = torch.randn(2, 1, 16, 128, 128)\n",
    "out = net(imgs)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7559)\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "import torch\n",
    "from datasets.dataset import AFMDataset, make_dataset\n",
    "from utils.criterion import modelLoss\n",
    "from torch.nn import BCELoss\n",
    "import torch\n",
    "a = AFMDataset(\"/home/supercgor/gitfile/ARAI/datasets/data/bulkice\", preload= False)\n",
    "# DL = torch.utils.data.DataLoader(\n",
    "#             a,\n",
    "#             batch_size=2,\n",
    "#             num_workers=0,\n",
    "#             pin_memory=True,\n",
    "#             shuffle=True)\n",
    "# imgs , gt, fl = next(iter(DL))\n",
    "imgs, gt, fl = a[0]\n",
    "net = CombineModel()\n",
    "result = net(imgs.unsqueeze(0))\n",
    "ml = modelLoss()\n",
    "LOSS = ml(gt.unsqueeze(0), gt.unsqueeze(0))\n",
    "print(LOSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.nn.MSELoss(reduction='none')\n",
    "b = torch.tensor([[0,0.0,0],[0,0.0,0]])\n",
    "c = torch.tensor([[1,1,1],[1,1,1]])\n",
    "a(b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.random.randn(5,5,5)\n",
    "a.take([1,2,3], axis = 0).shape\n",
    "print(a[1,2,3])\n",
    "print(a[(1,...,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class a():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __getitem__(self,ind):\n",
    "        print(ind)\n",
    "        \n",
    "b = a()\n",
    "b[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import ElemLayerMet\n",
    "elm = ElemLayerMet()\n",
    "print(elm[\"H\",0,'TP'])\n",
    "elm.get(\"TP\")\n",
    "MET = elm.get_met()\n",
    "import numpy as np\n",
    "a = np.random.randn(2,2)\n",
    "a[0,:] = [1,2]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import torch\n",
    "a = torch.randn((1,3,10,50))\n",
    "F.interpolate(a, size = (20,100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
