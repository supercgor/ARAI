{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "path = \"/Volumes/HEIU/data/bulkice_train.hdf5\"\n",
    "print(path)\n",
    "f = h5py.File(path, \"r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import AFMDataset\n",
    "path = \"/Volumes/HEIU/data/bulkice_train.hdf5\"\n",
    "\n",
    "dts = AFMDataset(path)\n",
    "fn, afm, label_type, label_pos = dts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(make_grid(afm.transpose(1,0)).permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['T180_13015'].afm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.dataset import z_sampler\n",
    "z_sampler(10, 4, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from dataset import transform as tf\n",
    "\n",
    "pics = torch.ones(1, 8, 100, 100) * 0.9\n",
    "pics[..., 50:, 0:50] = 0.1\n",
    "pics[..., :50, 50:] = 0.1\n",
    "\n",
    "transform = nn.Sequential(\n",
    "    tf.PixelShift(),\n",
    "    tf.Cutout(),\n",
    "    tf.ColorJitter(),\n",
    "    tf.Noisy(),\n",
    "    tf.Blur(),\n",
    ")\n",
    "t = time.time()\n",
    "for i in range(1):\n",
    "    pics = transform(pics)\n",
    "\n",
    "print(time.time() - t)\n",
    "\n",
    "\n",
    "T = torchvision.utils.make_grid(pics.transpose(1,0),pad_value = 0.5)\n",
    "plt.imshow(T.permute(1,2,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(100, 100)\n",
    "a[10:20, 10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.rand(100, 100, 3)\n",
    "b = torch.rand(100, 100) > 0.5\n",
    "a[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.ones((50, 50, 5))\n",
    "c = torch.rand((50, 50, 5, 3))\n",
    "mask = a.nonzero()\n",
    "print(mask.T.shape)\n",
    "a[tuple(mask.T)]\n",
    "(c[tuple(mask.T)] + mask) / (50, 50, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import poscar\n",
    "box_cls = torch.randint(0, 3, (3, 50, 50), dtype=torch.long)\n",
    "box_off = torch.rand((3, 50, 50, 3))\n",
    "print(poscar.boxncls2pos_torch(box_cls, box_off))\n",
    "print(poscar.boxncls2pos_np(box_cls.numpy(), box_off.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.metrics import Analyser\n",
    "from utils import poscar\n",
    "from dataset import AFMDataset\n",
    "path = \"/Volumes/HEIU/data/bulkice_train.hdf5\"\n",
    "\n",
    "dts = AFMDataset(path)\n",
    "fn, afm, label_type, label_pos = dts[0]\n",
    "pred_types, pred_pos = poscar.targ2pred(label_type, label_pos)\n",
    "a = torch.jit.script(Analyser())\n",
    "pred_types = pred_types.unsqueeze(0).repeat((10, 1, 1, 1, 1))\n",
    "pred_pos  = pred_pos.unsqueeze(0).repeat((10, 1, 1, 1, 1))\n",
    "label_type = label_type.unsqueeze(0).repeat((10, 1, 1, 1))\n",
    "label_pos = label_pos.unsqueeze(0).repeat((10, 1, 1, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "for i in range(1000):\n",
    "    a(pred_types, pred_pos, label_type, label_pos)\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((100, 100, 3))\n",
    "mask = torch.nonzero(a, as_tuple=True)\n",
    "a[mask[0], mask[1], mask[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from utils.metrics import ConfusionMatrixCounter\n",
    "cm = torch.randint(0, 100, (1, 2, 1, 3)).numpy()\n",
    "t = time.time()\n",
    "ConfusionMatrixCounter._count(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import unet_onehot\n",
    "m = unet_onehot()\n",
    "img = torch.rand((1, 1, 10, 100, 100))\n",
    "m(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from utils.metrics import Analyser\n",
    "from utils import poscar\n",
    "from dataset import AFMDataset\n",
    "path = \"/Volumes/HEIU/data/bulkice_train.hdf5\"\n",
    "\n",
    "dts = AFMDataset(path)\n",
    "fn, afm, label_type, label_pos = dts[0]\n",
    "print(fn)\n",
    "typ, final, _ = poscar.boxncls2pos_torch(label_type, label_pos)\n",
    "typ = typ.numpy()\n",
    "final = final.numpy()\n",
    "print(len(final))\n",
    "pos2 = np.concatenate([final[typ == i] for i in range(1,3)])\n",
    "poscar.save(\"test.poscar\", [3.0, 25.0, 25.0], ['O', 'H'], [48, 96], pos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = [\"1\", \"2\", \"3\"]\n",
    "np.array(a, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(\"9246.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @jit(nopython=True)\n",
    "def getWaterRotate(tag: np.ndarray, inv_ref: np.ndarray) -> np.ndarray:\n",
    "    O, H1, H2 = tag\n",
    "    H1, H2 = H1 - O, H2 - O\n",
    "    H1 = H1\n",
    "    H2 = H2\n",
    "    H3 = np.cross(H1, H2)\n",
    "    if H3[2] < 0:\n",
    "        tag[0] = H2\n",
    "        tag[1] = H1\n",
    "        tag[2] = -H3\n",
    "    else:\n",
    "        tag[0] = H1\n",
    "        tag[1] = H2\n",
    "        tag[2] = H3\n",
    "    return inv_ref @ tag\n",
    "\n",
    "ang = 104.52 / 180 * np.pi\n",
    "# v = np.array([0, 0, 1]) * 0.9572\n",
    "# u = np.array([np.sin(ang), 0, np.cos(ang)]) * 0.9572\n",
    "# r = np.cross(v, u)\n",
    "# print(v, u,r)\n",
    "initpos = np.array([\n",
    "        [ 0.         , 0.         , 0.        ],\n",
    "        [ 0.         , 0.         , 0.9572    ],\n",
    "        [ 0.9266272  , 0.         ,-0.23998721],\n",
    "    ])\n",
    "\n",
    "initdir = np.array([\n",
    "        [ 0.         , 0.         , 0.9572    ],\n",
    "        [ 0.9266272  , 0.         ,-0.23998721],\n",
    "        [ 0.         , 0.88696756 , 0.        ]\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "pos = np.asarray([[\n",
    "        [6.20233009,  6.80328286,  0.71540000], \n",
    "        [6.72382009,  6.43174286,  0.00390000], \n",
    "        [5.46304009,  6.20268286,  0.81010000]\n",
    "        ],\n",
    "        [\n",
    "        [14.10287009, 18.97821286,  0.47590000], \n",
    "        [14.35087009, 19.79591286,  0.04450000], \n",
    "        [13.24817009, 19.16231286,  0.86570000]\n",
    "        ]])\n",
    "\n",
    "def getRotate(final: np.ndarray, init: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Rotate init to final, init @ R = final\n",
    "\n",
    "    Args:\n",
    "        final (np.ndarray): N 3 3\n",
    "        init (np.ndarray): 3 3\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: N 3 3\n",
    "    \"\"\"\n",
    "    init = np.linalg.inv(init)\n",
    "    final = final - final[:, (0,), :]\n",
    "    cross = np.cross(final[:,1], final[:, 2])\n",
    "    \n",
    "    final = np.stack([final[:,1], final[:, 2], cross], axis=1)\n",
    "    R = np.einsum(\"ij, Njk -> Nik\", init, final)\n",
    "    # print(np.linalg.det(R))\n",
    "    return R\n",
    "    \n",
    "R = getRotate(pos, initdir)\n",
    "\n",
    "R1 = R[0]\n",
    "print(initpos @ R1)\n",
    "# print(np.einsum(\"Nij, kj -> Nki\", R, initpos))\n",
    "\n",
    "import time\n",
    "t = time.time()\n",
    "R2 = getWaterRotate(pos[0].copy(), np.linalg.inv(initdir))\n",
    "print(time.time() - t)\n",
    "\n",
    "print(initpos @ R2[(0,2,1),:])\n",
    "print(initpos @ R2)\n",
    "# print(pos[0] - pos[0,(0,), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.cross(R1[0], R1[1]) - R1[2])\n",
    "print(np.cross(R2[0], R2[1]) - R2[2])\n",
    "print(R1)\n",
    "print(R2)\n",
    "print(R2[(0,-2,-1),:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import VAEunet\n",
    "import utils\n",
    "net = VAEunet(channel_mult = (1, 2, 3, 4))\n",
    "temp = torch.rand((1, 1))\n",
    "print(\"\\n\".join(utils.model.model_structure(net)))\n",
    "image = torch.rand((1, 10, 4,25, 25))\n",
    "mu, nu, x = net(image, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import AFMGenDataset\n",
    "dts = AFMGenDataset(\"/Volumes/HEIU/data/ice_8_4A/ice_8_4A_train.hdf5\")\n",
    "name, box4a, box8a = dts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(box4a[...,0].nonzero()))\n",
    "print(len(box8a[...,0].nonzero()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.zeros(10)\n",
    "a[(0,1,2),] = 1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmatch(pred: Tensor, targ: Tensor, cutoff: float) -> tuple[Tensor]:\n",
    "    dis = torch.cdist(targ, pred)\n",
    "    dis = (dis < cutoff).nonzero()\n",
    "    dis = dis[:, (1, 0)]\n",
    "    unique, idx, counts = torch.unique(dis[...,1], sorted=True, return_inverse=True, return_counts=True)\n",
    "    ind_sorted = torch.argsort(idx, stable=True)\n",
    "    cum_sum = counts.cumsum(0)\n",
    "    cum_sum = torch.cat((torch.tensor([0]), cum_sum[:-1]))\n",
    "    first_indicies = ind_sorted[cum_sum]\n",
    "    dis = dis[first_indicies]\n",
    "    return dis\n",
    "\n",
    "import torch\n",
    "b = torch.rand(1000, 3)\n",
    "c = torch.rand(100, 3)\n",
    "t = time.time()\n",
    "for i in range(1000):\n",
    "    reg = argmatch(b, c, 0.1)\n",
    "print(time.time() - t)\n",
    "reg = argmatch(b, c, 0.1).T\n",
    "print(reg)\n",
    "print(reg[0].unique().shape, reg[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "a = torch.tensor([1.0])\n",
    "b = a.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [1]\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.zeros((10,10))\n",
    "b = []\n",
    "b.extend(a)\n",
    "\n",
    "import torch\n",
    "# 0.0, 0.5, 1.0; 1.0, 2.0, 3.0\n",
    "# 0.0, 0.5, 1.0; 0.0, 1.0, 2.0\n",
    "a = torch.linspace(0, 1, 3)[None, None, :].repeat(1, 3, 1)\n",
    "a[:, 1] += 1\n",
    "b = torch.linspace(1, 3, 3)[None, None, :].repeat(1, 3, 1)\n",
    "c.arccos_().div_(torch.pi).mul_(180)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "def f():\n",
    "    print(\"Zzz\")\n",
    "    time.sleep(5)\n",
    "\n",
    "p = mp.Pool(6)\n",
    "\n",
    "_ = [p.apply_async(f) for i in range(12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((3,3))\n",
    "a[0] = 1\n",
    "b = np.random.rand(3,3)\n",
    "print(a)\n",
    "print(b)\n",
    "print(np.matmul(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import AFMGenDataset\n",
    "from utils import functional\n",
    "import torch\n",
    "a  = AFMGenDataset(\"/Users/supercgor/Documents/data/ice_8_4A/ice_8_4A_small_train.hdf5\")\n",
    "filename ,inp, targ, emb = a[0]\n",
    "conf, pos, rot = functional.box2orgvec(targ, 0.5, 1.0, (25.0,25.0, 4.0), sort = False, nms = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "pos = torch.tensor([[23.5279, 19.8613,  0.7911]])\n",
    "rot = torch.tensor([[[ 0.1403,  0.2997, -0.9437],\n",
    "        [-0.5516,  0.8152,  0.1769],\n",
    "        [ 0.8223,  0.4957,  0.2797]]])\n",
    "from utils.functional import getWaterRotate, makewater\n",
    "print(pos.shape)\n",
    "print(makewater(pos,rot))\n",
    "initdir = np.array([\n",
    "        [ 0.         , 0.         , 0.9572    ],\n",
    "        [ 0.9266272  , 0.         ,-0.23998721],\n",
    "        [ 0.         , 0.88696756 , 0.        ]\n",
    "        ])\n",
    "invref = np.linalg.inv(initdir)\n",
    "pos = np.array([[23.52787009, 19.86131286,  4.7911    ],\n",
    " [24.31487009, 20.33571286,  5.0589    ],\n",
    " [23.46057009, 20.02011286,  3.8495    ]])\n",
    "print(pos-pos[0])\n",
    "getWaterRotate(pos, invref)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import AFMGenDataset\n",
    "from utils import functional\n",
    "import torch\n",
    "a  = AFMGenDataset(\"/Users/supercgor/Documents/data/ice_8_4A/ice_8_4A_small_train.hdf5\")\n",
    "filename ,inp, targ, emb = a[0]\n",
    "\n",
    "targ2 = functional.box2box(targ)\n",
    "\n",
    "torch.allclose(targ, targ2, rtol = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones_like(inp, (1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = [1, 2, 3]\n",
    "b = list(map(lambda x: x + 1, a))\n",
    "print(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import VAELatEmb\n",
    "a = VAELatEmb()\n",
    "b = torch.randn((2, 10, 4, 25, 25))\n",
    "c = torch.randn((2, 1))\n",
    "out, mu, nu = a(b, c)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randn((2, 4, 4))\n",
    "torch.where(a[...,(0,)]> 0.5, torch.zeros_like(a), a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ZVarAFM\n",
    "from utils.library import encodeWater\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = \"/Users/supercgor/Documents/data/ice_16A/ice_16A_train.hdf5\"\n",
    "\n",
    "dts = ZVarAFM(path, (25, 25, 4))\n",
    "file_name, inpbox, outbox, emb = dts[0]\n",
    "\n",
    "ind = inpbox[...,0] == 1\n",
    "\n",
    "fea = inpbox[ind.nonzero(as_tuple=True)][:,1:]\n",
    "fea[:,:3] += ind.nonzero()\n",
    "make = encodeWater(fea)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "\n",
    "ax.title.set_text('inpbox')\n",
    "# 取出矩陣的每一列的數據\n",
    "ax.scatter(make[...,0], make[...,1], make[...,2], c='r', marker='o')\n",
    "ax.scatter(make[...,3], make[...,4], make[...,5], c='b', marker='o')\n",
    "ax.scatter(make[...,6], make[...,7], make[...,8], c='b', marker='o')\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "ind = outbox[...,0] == 1\n",
    "\n",
    "fea = outbox[ind.nonzero(as_tuple=True)][:,1:]\n",
    "fea[:,:3] += ind.nonzero()\n",
    "make = encodeWater(fea)\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "ax.title.set_text('outbox')\n",
    "# 取出矩陣的每一列的數據\n",
    "ax.scatter(make[...,0], make[...,1], make[...,2], c='r', marker='o')\n",
    "ax.scatter(make[...,3], make[...,4], make[...,5], c='b', marker='o')\n",
    "ax.scatter(make[...,6], make[...,7], make[...,8], c='b', marker='o')\n",
    "\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ZVarAFM\n",
    "from model import VAELatEmb_v2\n",
    "from utils.library import encodeWater\n",
    "from utils.model import model_structure\n",
    "path = \"/Users/supercgor/Documents/data/ice_16A/ice_16A_train.hdf5\"\n",
    "\n",
    "dts = ZVarAFM(path, (25, 25, 4))\n",
    "file_name, inpbox, outbox, emb = dts[0]\n",
    "\n",
    "inpbox = inpbox[None,...].permute(0, 4, 3, 1, 2)\n",
    "emb = emb[None, ...]\n",
    "net = VAELatEmb_v2()\n",
    "\n",
    "out = net(inpbox, emb)\n",
    "print(\"\\n\".join(model_structure(net)))\n",
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.clip(x-6, 0, None)%4/10 + np.clip(x,0,6)/10\n",
    "\n",
    "import numpy as np\n",
    "a = np.linspace(0, 30, 1000)\n",
    "b = f(a)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import unet_water\n",
    "import torch\n",
    "from utils.model import model_structure\n",
    "net = unet_water(num_res_blocks=(3,3))\n",
    "a = torch.rand((1, 1, 10, 100, 100))\n",
    "b = torch.rand((1, 1))\n",
    "net(a,b).shape\n",
    "model_structure(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([100, 100, 10], dtype=np.float32)\n",
    "np.ceil(a/2, out= a)\n",
    "print(a)\n",
    "np.ceil(a/2, out= a)\n",
    "print(a)\n",
    "np.ceil(a/2, out= a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(10):\n",
    "    pass\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_as_water(pos_o, pos_h):\n",
    "    # N, 3 2N, 3 -> N, 9\n",
    "    dis = torch.cdist(pos_o, pos_h)\n",
    "    dis = torch.topk(dis, 2, dim = 1, largest = False).indices\n",
    "    return torch.cat([pos_o,pos_h[dis].view(-1, 6)], dim = -1)\n",
    "    \n",
    "from utils import poscar, library\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "card = poscar.load(\"/Users/supercgor/Library/CloudStorage/OneDrive-北京大学/PKU/Data/bulkice/label/T160_14805.poscar\")\n",
    "real = np.diag(card['lattice'])\n",
    "dis = card['pos'] * real\n",
    "print(card.keys())\n",
    "split_ = card['ion_num']\n",
    "O, H = np.split(dis, np.cumsum(split_)[:-1], axis=0)\n",
    "print(O.shape, H.shape)\n",
    "water = group_as_water(torch.from_numpy(O), torch.from_numpy(H))\n",
    "code_water = library.decodeWater(water)\n",
    "print(code_water[1], water[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.library import encodeWater\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "path = \"/Users/supercgor/Documents/data/union-water-data_train.hdf5\"\n",
    "\n",
    "with h5py.File(path, \"r\") as f:\n",
    "    make = f['basal_T160_03_7_1_0']['pos'][()]\n",
    "    \n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "\n",
    "ax.title.set_text('inpbox')\n",
    "# 取出矩陣的每一列的數據\n",
    "ax.scatter(make[...,0], make[...,1], make[...,2], c='r', marker='o')\n",
    "ax.scatter(make[...,3], make[...,4], make[...,5], c='b', marker='o')\n",
    "ax.scatter(make[...,6], make[...,7], make[...,8], c='b', marker='o')\n",
    "ax.set_aspect('equal', adjustable='box')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1.999\n",
    "a % 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.ndisc import n_layer_disc\n",
    "from utils import model_structure\n",
    "net = n_layer_disc(1, 32, [8, 4, 2, 1], [1, 2])\n",
    "print(net)\n",
    "image = torch.randn(1, 1, 10, 100, 100)\n",
    "print(net(image).shape)\n",
    "\n",
    "print(\"\\n\".join(model_structure(net)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "def knn_point(nsample, xyz, new_xyz):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        nsample: max sample number in local region\n",
    "        xyz: all points, [B, N, C]\n",
    "        new_xyz: query points, [B, S, C]\n",
    "    Return:\n",
    "        group_idx: grouped points index, [B, S, nsample]\n",
    "    \"\"\"\n",
    "    sqrdists = square_distance(new_xyz, xyz)\n",
    "    _, group_idx = torch.topk(sqrdists, nsample, dim = -1, largest=False, sorted=False)\n",
    "    return group_idx\n",
    "\n",
    "def square_distance(src, dst):\n",
    "    \"\"\"\n",
    "    Calculate Euclid distance between each two points.\n",
    "    src^T * dst = xn * xm + yn * ym + zn * zm\n",
    "    sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;\n",
    "    sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;\n",
    "    dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2\n",
    "         = sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst\n",
    "    Input:\n",
    "        src: source points, [B, N, C]\n",
    "        dst: target points, [B, M, C]\n",
    "    Output:\n",
    "        dist: per-point square distance, [B, N, M]\n",
    "    \"\"\"\n",
    "    B, N, _ = src.shape\n",
    "    _, M, _ = dst.shape\n",
    "    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))\n",
    "    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n",
    "    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n",
    "    return dist  \n",
    "\n",
    "def get_graph_feature(coor_q, x_q, coor_k, x_k):\n",
    "\n",
    "        # coor: bs, 3, np, x: bs, c, np\n",
    "\n",
    "        k = 16\n",
    "        batch_size = x_k.size(0)\n",
    "        num_points_k = x_k.size(2)\n",
    "        num_points_q = x_q.size(2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "#             _, idx = knn(coor_k, coor_q)  # bs k np\n",
    "            idx = knn_point(k, coor_k.transpose(-1, -2).contiguous(), coor_q.transpose(-1, -2).contiguous()) # B G M\n",
    "            idx = idx.transpose(-1, -2).contiguous()\n",
    "            assert idx.shape[1] == k\n",
    "            idx_base = torch.arange(0, batch_size, device=x_q.device).view(-1, 1, 1) * num_points_k\n",
    "            idx = idx + idx_base\n",
    "            idx = idx.view(-1)\n",
    "        num_dims = x_k.size(1)\n",
    "        x_k = x_k.transpose(2, 1).contiguous()\n",
    "        feature = x_k.view(batch_size * num_points_k, -1)[idx, :]\n",
    "        feature = feature.view(batch_size, k, num_points_q, num_dims).permute(0, 3, 2, 1).contiguous()\n",
    "        x_q = x_q.view(batch_size, num_dims, num_points_q, 1).expand(-1, -1, -1, k)\n",
    "        feature = torch.cat((feature - x_q, x_q), dim=1)\n",
    "        return feature\n",
    "\n",
    "l1 = nn.Sequential(nn.Conv2d(16, 32, kernel_size=1, bias=False),\n",
    "                                   nn.GroupNorm(4, 32),\n",
    "                                   nn.LeakyReLU(negative_slope=0.2)\n",
    "                                   )\n",
    "\n",
    "coor = torch.rand(10, 3, 100)\n",
    "fea = torch.rand(10, 8, 100)\n",
    "coor2 = torch.rand(10, 3, 70)\n",
    "fea2 = torch.rand(10, 8, 70)\n",
    "fea  = get_graph_feature(coor, fea, coor2, fea2)\n",
    "print(fea.shape)\n",
    "l1(fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2048, 3]) 512\n",
      "torch.Size([10, 512, 512])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/supercgor/Documents/ARAI/test.ipynb Cell 51\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/supercgor/Documents/ARAI/test.ipynb#Y101sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m net \u001b[39m=\u001b[39m DGCNN_Grouper()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/supercgor/Documents/ARAI/test.ipynb#Y101sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand((\u001b[39m10\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m2048\u001b[39m))\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/supercgor/Documents/ARAI/test.ipynb#Y101sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m net(a)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/point/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/point/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/supercgor/Documents/ARAI/test.ipynb Cell 51\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/supercgor/Documents/ARAI/test.ipynb#Y101sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(f)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/supercgor/Documents/ARAI/test.ipynb#Y101sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m f \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/supercgor/Documents/ARAI/test.ipynb#Y101sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m coor_q, f_q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfps_downsample(coor, f, \u001b[39m512\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/supercgor/Documents/ARAI/test.ipynb#Y101sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_graph_feature(coor_q, f_q, coor, f)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/supercgor/Documents/ARAI/test.ipynb#Y101sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(f)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from pytorch3d.ops import sample_farthest_points, knn_points, knn_gather, ball_query\n",
    "\n",
    "def knn_point(nsample, xyz, new_xyz):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        nsample: max sample number in local region\n",
    "        xyz: all points, [B, N, C]\n",
    "        new_xyz: query points, [B, S, C]\n",
    "    Return:\n",
    "        group_idx: grouped points index, [B, S, nsample]\n",
    "    \"\"\"\n",
    "    sqrdists = square_distance(new_xyz, xyz)\n",
    "    _, group_idx = torch.topk(sqrdists, nsample, dim = -1, largest=False, sorted=False)\n",
    "    return group_idx\n",
    "\n",
    "def square_distance(src, dst):\n",
    "    \"\"\"\n",
    "    Calculate Euclid distance between each two points.\n",
    "    src^T * dst = xn * xm + yn * ym + zn * zm\n",
    "    sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;\n",
    "    sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;\n",
    "    dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2\n",
    "         = sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst\n",
    "    Input:\n",
    "        src: source points, [B, N, C]\n",
    "        dst: target points, [B, M, C]\n",
    "    Output:\n",
    "        dist: per-point square distance, [B, N, M]\n",
    "    \"\"\"\n",
    "    B, N, _ = src.shape\n",
    "    _, M, _ = dst.shape\n",
    "    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))\n",
    "    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n",
    "    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n",
    "    return dist  \n",
    "\n",
    "\n",
    "class DGCNN_Grouper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        K has to be 16\n",
    "        '''\n",
    "        self.input_trans = nn.Conv1d(3, 8, 1)\n",
    "\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(16, 32, kernel_size=1, bias=False),\n",
    "                                   nn.GroupNorm(4, 32),\n",
    "                                   nn.LeakyReLU(negative_slope=0.2)\n",
    "                                   )\n",
    "\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n",
    "                                   nn.GroupNorm(4, 64),\n",
    "                                   nn.LeakyReLU(negative_slope=0.2)\n",
    "                                   )\n",
    "\n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(128, 64, kernel_size=1, bias=False),\n",
    "                                   nn.GroupNorm(4, 64),\n",
    "                                   nn.LeakyReLU(negative_slope=0.2)\n",
    "                                   )\n",
    "\n",
    "        self.layer4 = nn.Sequential(nn.Conv2d(128, 128, kernel_size=1, bias=False),\n",
    "                                   nn.GroupNorm(4, 128),\n",
    "                                   nn.LeakyReLU(negative_slope=0.2)\n",
    "                                   )\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def fps_downsample(coor, x, num_group):\n",
    "        xyz = coor.transpose(1, 2).contiguous() # b, n, 3\n",
    "        print(xyz.shape, num_group)\n",
    "        fps_points, fps_idx = sample_farthest_points(xyz, K = num_group)\n",
    "        BQ = ball_query(fps_points, xyz, radius=0.01, K = num_group, return_nn = False)\n",
    "        \n",
    "        ball_features, ball_idx = BQ.dists, BQ.idx\n",
    "        print(ball_features.shape)\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_graph_feature(coor_q, x_q, coor_k, x_k):\n",
    "\n",
    "        # coor: bs, 3, np, x: bs, c, np\n",
    "\n",
    "        k = 16\n",
    "        batch_size = x_k.size(0)\n",
    "        num_points_k = x_k.size(2)\n",
    "        num_points_q = x_q.size(2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "#             _, idx = knn(coor_k, coor_q)  # bs k np\n",
    "            idx = knn_points(coor_k.transpose(-1, -2).contiguous(), coor_q.transpose(-1, -2).contiguous(), K = k) # B G M\n",
    "            idx = idx.idx.transpose(-1, -2).contiguous()\n",
    "            assert idx.shape[1] == k\n",
    "            idx_base = torch.arange(0, batch_size, device=x_q.device).view(-1, 1, 1) * num_points_k\n",
    "            idx = idx + idx_base\n",
    "            idx = idx.view(-1)\n",
    "        num_dims = x_k.size(1)\n",
    "        x_k = x_k.transpose(2, 1).contiguous()\n",
    "        feature = x_k.view(batch_size * num_points_k, -1)[idx, :]\n",
    "        feature = feature.view(batch_size, k, num_points_q, num_dims).permute(0, 3, 2, 1).contiguous()\n",
    "        x_q = x_q.view(batch_size, num_dims, num_points_q, 1).expand(-1, -1, -1, k)\n",
    "        feature = torch.cat((feature - x_q, x_q), dim=1)\n",
    "        return feature\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x: bs, 3, np\n",
    "\n",
    "        # bs 3 N(128)   bs C(224)128 N(128)\n",
    "        coor = x\n",
    "        f = self.input_trans(x)\n",
    "\n",
    "        f = self.get_graph_feature(coor, f, coor, f)\n",
    "        f = self.layer1(f)\n",
    "        f = f.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        coor_q, f_q = self.fps_downsample(coor, f, 512)\n",
    "        f = self.get_graph_feature(coor_q, f_q, coor, f)\n",
    "        f = self.layer2(f)\n",
    "        f = f.max(dim=-1, keepdim=False)[0]\n",
    "        coor = coor_q\n",
    "\n",
    "        f = self.get_graph_feature(coor, f, coor, f)\n",
    "        f = self.layer3(f)\n",
    "        f = f.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        coor_q, f_q = self.fps_downsample(coor, f, 128)\n",
    "        f = self.get_graph_feature(coor_q, f_q, coor, f)\n",
    "        f = self.layer4(f)\n",
    "        f = f.max(dim=-1, keepdim=False)[0]\n",
    "        coor = coor_q\n",
    "\n",
    "        return coor, f\n",
    "    \n",
    "net = DGCNN_Grouper()\n",
    "a = torch.rand((10, 3, 2048))\n",
    "net(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from pytorch3d.ops import sample_farthest_points, knn_points, knn_gather\n",
    "# from knn_cuda import KNN\n",
    "# knn = KNN(k=8, transpose_mode=False)\n",
    "\n",
    "def knn_point(nsample, xyz, new_xyz):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        nsample: max sample number in local region\n",
    "        xyz: all points, [B, N, C]\n",
    "        new_xyz: query points, [B, S, C]\n",
    "    Return:\n",
    "        group_idx: grouped points index, [B, S, nsample]\n",
    "    \"\"\"\n",
    "    sqrdists = square_distance(new_xyz, xyz)\n",
    "    _, group_idx = torch.topk(sqrdists, nsample, dim = -1, largest=False, sorted=False)\n",
    "    return group_idx\n",
    "\n",
    "def square_distance(src, dst):\n",
    "    \"\"\"\n",
    "    Calculate Euclid distance between each two points.\n",
    "    src^T * dst = xn * xm + yn * ym + zn * zm;\n",
    "    sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;\n",
    "    sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;\n",
    "    dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2\n",
    "         = sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst\n",
    "    Input:\n",
    "        src: source points, [B, N, C]\n",
    "        dst: target points, [B, M, C]\n",
    "    Output:\n",
    "        dist: per-point square distance, [B, N, M]\n",
    "    \"\"\"\n",
    "    B, N, _ = src.shape\n",
    "    _, M, _ = dst.shape\n",
    "    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))\n",
    "    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n",
    "    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n",
    "    return dist   \n",
    "\n",
    "def get_knn_index(coor_q, coor_k=None):\n",
    "    coor_k = coor_k if coor_k is not None else coor_q\n",
    "    # coor: bs, 3, np\n",
    "    batch_size, _, num_points = coor_q.size()\n",
    "    num_points_k = coor_k.size(2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "#         _, idx = knn(coor_k, coor_q)  # bs k np\n",
    "        idx = knn_point(8, coor_k.transpose(-1, -2).contiguous(), coor_q.transpose(-1, -2).contiguous()) # B G M\n",
    "        idx = idx.transpose(-1, -2).contiguous()\n",
    "        idx_base = torch.arange(0, batch_size, device=coor_q.device).view(-1, 1, 1) * num_points_k\n",
    "        idx = idx + idx_base\n",
    "        idx = idx.view(-1)\n",
    "    \n",
    "    return idx  # bs*k*np\n",
    "\n",
    "def get_graph_feature(x, knn_index, x_q=None):\n",
    "\n",
    "        #x: bs, np, c, knn_index: bs*k*np\n",
    "        k = 8\n",
    "        batch_size, num_points, num_dims = x.size()\n",
    "        num_query = x_q.size(1) if x_q is not None else num_points\n",
    "        feature = x.view(batch_size * num_points, num_dims)[knn_index, :]\n",
    "        feature = feature.view(batch_size, k, num_query, num_dims)\n",
    "        x = x_q if x_q is not None else x\n",
    "        x = x.view(batch_size, 1, num_query, num_dims).expand(-1, k, -1, -1)\n",
    "        feature = torch.cat((feature - x, x), dim=-1)\n",
    "        return feature  # b k np c\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim, out_dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim = dim\n",
    "        self.out_dim = out_dim\n",
    "        head_dim = out_dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.q_map = nn.Linear(dim, out_dim, bias=qkv_bias)\n",
    "        self.k_map = nn.Linear(dim, out_dim, bias=qkv_bias)\n",
    "        self.v_map = nn.Linear(dim, out_dim, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "\n",
    "        self.proj = nn.Linear(out_dim, out_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, q, v):\n",
    "        B, N, _ = q.shape\n",
    "        C = self.out_dim\n",
    "        k = v\n",
    "        NK = k.size(1)\n",
    "\n",
    "        q = self.q_map(q).view(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        k = self.k_map(k).view(B, NK, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        v = self.v_map(v).view(B, NK, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, dim_q = None, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.self_attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        dim_q = dim_q or dim\n",
    "        self.norm_q = norm_layer(dim_q)\n",
    "        self.norm_v = norm_layer(dim)\n",
    "        self.attn = CrossAttention(\n",
    "            dim, dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        self.knn_map = nn.Sequential(\n",
    "            nn.Linear(dim * 2, dim),\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "\n",
    "        self.merge_map = nn.Linear(dim*2, dim)\n",
    "\n",
    "        self.knn_map_cross = nn.Sequential(\n",
    "            nn.Linear(dim * 2, dim),\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "\n",
    "        self.merge_map_cross = nn.Linear(dim*2, dim)\n",
    "\n",
    "    def forward(self, q, v, self_knn_index=None, cross_knn_index=None):\n",
    "        # q = q + self.drop_path(self.self_attn(self.norm1(q)))\n",
    "        norm_q = self.norm1(q)\n",
    "        q_1 = self.self_attn(norm_q)\n",
    "\n",
    "        if self_knn_index is not None:\n",
    "            knn_f = get_graph_feature(norm_q, self_knn_index)\n",
    "            knn_f = self.knn_map(knn_f)\n",
    "            knn_f = knn_f.max(dim=1, keepdim=False)[0]\n",
    "            q_1 = torch.cat([q_1, knn_f], dim=-1)\n",
    "            q_1 = self.merge_map(q_1)\n",
    "        \n",
    "        q = q + self.drop_path(q_1)\n",
    "\n",
    "        norm_q = self.norm_q(q)\n",
    "        norm_v = self.norm_v(v)\n",
    "        q_2 = self.attn(norm_q, norm_v)\n",
    "\n",
    "        if cross_knn_index is not None:\n",
    "            knn_f = get_graph_feature(norm_v, cross_knn_index, norm_q)\n",
    "            knn_f = self.knn_map_cross(knn_f)\n",
    "            knn_f = knn_f.max(dim=1, keepdim=False)[0]\n",
    "            q_2 = torch.cat([q_2, knn_f], dim=-1)\n",
    "            q_2 = self.merge_map_cross(q_2)\n",
    "\n",
    "        q = q + self.drop_path(q_2)\n",
    "\n",
    "        # q = q + self.drop_path(self.attn(self.norm_q(q), self.norm_v(v)))\n",
    "        q = q + self.drop_path(self.mlp(self.norm2(q)))\n",
    "        return q\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "\n",
    "        self.knn_map = nn.Sequential(\n",
    "            nn.Linear(dim * 2, dim),\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "\n",
    "        self.merge_map = nn.Linear(dim*2, dim)\n",
    "\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, knn_index = None):\n",
    "        # x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        norm_x = self.norm1(x)\n",
    "        x_1 = self.attn(norm_x)\n",
    "\n",
    "        if knn_index is not None:\n",
    "            knn_f = get_graph_feature(norm_x, knn_index)\n",
    "            knn_f = self.knn_map(knn_f)\n",
    "            knn_f = knn_f.max(dim=1, keepdim=False)[0]\n",
    "            x_1 = torch.cat([x_1, knn_f], dim=-1)\n",
    "            x_1 = self.merge_map(x_1)\n",
    "        \n",
    "        x = x + self.drop_path(x_1)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class PCTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for point cloud completion\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans=3, embed_dim=768, depth=[6, 6], num_heads=6, mlp_ratio=2., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                        num_query = 224, knn_layer = -1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        \n",
    "        self.knn_layer = knn_layer\n",
    "\n",
    "        self.grouper = DGCNN_Grouper()  # B 3 N to B C(3) N(128) and B C(128) N(128)\n",
    "\n",
    "        self.pos_embed = nn.Sequential(\n",
    "            nn.Conv1d(in_chans, 128, 1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Conv1d(128, embed_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Conv1d(128, embed_dim, 1),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Conv1d(embed_dim, embed_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate)\n",
    "            for i in range(depth[0])])\n",
    "\n",
    "        # self.increase_dim = nn.Sequential(\n",
    "        #     nn.Linear(embed_dim,1024),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Linear(1024, 1024)\n",
    "        # )\n",
    "\n",
    "        self.increase_dim = nn.Sequential(\n",
    "            nn.Conv1d(embed_dim, 1024, 1),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Conv1d(1024, 1024, 1)\n",
    "        )\n",
    "\n",
    "        self.num_query = num_query\n",
    "        self.coarse_pred = nn.Sequential(\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 3 * num_query)\n",
    "        )\n",
    "        self.mlp_query = nn.Sequential(\n",
    "            nn.Conv1d(1024 + 3, 1024, 1),\n",
    "            # nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Conv1d(1024, 1024, 1),\n",
    "            # nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Conv1d(1024, embed_dim, 1)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.ModuleList([\n",
    "            DecoderBlock(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate)\n",
    "            for i in range(depth[1])])\n",
    "\n",
    "\n",
    "    def pos_encoding_sin_wave(self, coor):\n",
    "        # ref to https://arxiv.org/pdf/2003.08934v2.pdf\n",
    "        D = 64 #\n",
    "        # normal the coor into [-1, 1], batch wise\n",
    "        normal_coor = 2 * ((coor - coor.min()) / (coor.max() - coor.min())) - 1 \n",
    "\n",
    "        # define sin wave freq\n",
    "        freqs = torch.arange(D, dtype=torch.float).cuda() \n",
    "        freqs = np.pi * (2**freqs)       \n",
    "\n",
    "        freqs = freqs.view(*[1]*len(normal_coor.shape), -1) # 1 x 1 x 1 x D\n",
    "        normal_coor = normal_coor.unsqueeze(-1) # B x 3 x N x 1\n",
    "        k = normal_coor * freqs # B x 3 x N x D\n",
    "        s = torch.sin(k) # B x 3 x N x D\n",
    "        c = torch.cos(k) # B x 3 x N x D\n",
    "        x = torch.cat([s,c], -1) # B x 3 x N x 2D\n",
    "        pos = x.transpose(-1,-2).reshape(coor.shape[0], -1, coor.shape[-1]) # B 6D N\n",
    "        # zero_pad = torch.zeros(x.size(0), 2, x.size(-1)).cuda()\n",
    "        # pos = torch.cat([x, zero_pad], dim = 1)\n",
    "        # pos = self.pos_embed_wave(x)\n",
    "        return pos\n",
    "\n",
    "    def forward(self, inpc):\n",
    "        '''\n",
    "            inpc : input incomplete point cloud with shape B N(2048) C(3)\n",
    "        '''\n",
    "        # build point proxy\n",
    "        bs = inpc.size(0)\n",
    "        coor, f = self.grouper(inpc.transpose(1,2).contiguous()) \n",
    "        knn_index = get_knn_index(coor)\n",
    "        # NOTE: try to use a sin wave  coor B 3 N, change the pos_embed input dim\n",
    "        # pos = self.pos_encoding_sin_wave(coor).transpose(1,2)\n",
    "        pos =  self.pos_embed(coor).transpose(1,2)\n",
    "        x = self.input_proj(f).transpose(1,2)\n",
    "        # cls_pos = self.cls_pos.expand(bs, -1, -1)\n",
    "        # cls_token = self.cls_pos.expand(bs, -1, -1)\n",
    "        # x = torch.cat([cls_token, x], dim=1)\n",
    "        # pos = torch.cat([cls_pos, pos], dim=1)\n",
    "        # encoder\n",
    "        for i, blk in enumerate(self.encoder):\n",
    "            if i < self.knn_layer:\n",
    "                x = blk(x + pos, knn_index)   # B N C\n",
    "            else:\n",
    "                x = blk(x + pos)\n",
    "        # build the query feature for decoder\n",
    "        # global_feature  = x[:, 0] # B C\n",
    "\n",
    "        global_feature = self.increase_dim(x.transpose(1,2)) # B 1024 N \n",
    "        global_feature = torch.max(global_feature, dim=-1)[0] # B 1024\n",
    "\n",
    "        coarse_point_cloud = self.coarse_pred(global_feature).reshape(bs, -1, 3)  #  B M C(3)\n",
    "\n",
    "        new_knn_index = get_knn_index(coarse_point_cloud.transpose(1, 2).contiguous())\n",
    "        cross_knn_index = get_knn_index(coor_k=coor, coor_q=coarse_point_cloud.transpose(1, 2).contiguous())\n",
    "\n",
    "        query_feature = torch.cat([\n",
    "            global_feature.unsqueeze(1).expand(-1, self.num_query, -1), \n",
    "            coarse_point_cloud], dim=-1) # B M C+3 \n",
    "        q = self.mlp_query(query_feature.transpose(1,2)).transpose(1,2) # B M C \n",
    "        # decoder\n",
    "        for i, blk in enumerate(self.decoder):\n",
    "            if i < self.knn_layer:\n",
    "                q = blk(q, x, new_knn_index, cross_knn_index)   # B M C\n",
    "            else:\n",
    "                q = blk(q, x)\n",
    "\n",
    "        return q, coarse_point_cloud\n",
    "    \n",
    "net = PCTransformer()\n",
    "a = torch.rand(10, 128, 3)\n",
    "net(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch3d\n",
    "a = torch.randn(10, 100, 3)\n",
    "pytorch3d.ops.sample_farthest_points(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "import numpy as np\n",
    "from pytorch3d.ops import sample_farthest_points, knn_points, knn_gather, ball_query\n",
    "\n",
    "\n",
    "def timeit(tag, t):\n",
    "    print(\"{}: {}s\".format(tag, time() - t))\n",
    "    return time()\n",
    "\n",
    "def pc_normalize(pc):\n",
    "    l = pc.shape[0]\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    pc = pc / m\n",
    "    return pc\n",
    "\n",
    "def square_distance(src, dst):\n",
    "    \"\"\"\n",
    "    Calculate Euclid distance between each two points.\n",
    "\n",
    "    src^T * dst = xn * xm + yn * ym + zn * zm；\n",
    "    sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;\n",
    "    sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;\n",
    "    dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2\n",
    "         = sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst\n",
    "\n",
    "    Input:\n",
    "        src: source points, [B, N, C]\n",
    "        dst: target points, [B, M, C]\n",
    "    Output:\n",
    "        dist: per-point square distance, [B, N, M]\n",
    "    \"\"\"\n",
    "    B, N, _ = src.shape\n",
    "    _, M, _ = dst.shape\n",
    "    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))\n",
    "    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n",
    "    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n",
    "    return dist\n",
    "\n",
    "\n",
    "def index_points(points, idx):\n",
    "    \"\"\"\n",
    "\n",
    "    Input:\n",
    "        points: input points data, [B, N, C]\n",
    "        idx: sample index data, [B, S]\n",
    "    Return:\n",
    "        new_points:, indexed points data, [B, S, C]\n",
    "    \"\"\"\n",
    "    device = points.device\n",
    "    B = points.shape[0]\n",
    "    view_shape = list(idx.shape)\n",
    "    view_shape[1:] = [1] * (len(view_shape) - 1)\n",
    "    repeat_shape = list(idx.shape)\n",
    "    repeat_shape[0] = 1\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape)\n",
    "    new_points = points[batch_indices, idx, :]\n",
    "    return new_points\n",
    "\n",
    "\n",
    "def farthest_point_sample(xyz, npoint):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: pointcloud data, [B, N, 3]\n",
    "        npoint: number of samples\n",
    "    Return:\n",
    "        centroids: sampled pointcloud index, [B, npoint]\n",
    "    \"\"\"\n",
    "    centroids = sample_farthest_points(xyz, K = npoint)[1]\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def query_ball_point(radius, nsample, xyz, new_xyz):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        radius: local region radius\n",
    "        nsample: max sample number in local region\n",
    "        xyz: all points, [B, N, 3]\n",
    "        new_xyz: query points, [B, S, 3]\n",
    "    Return:\n",
    "        group_idx: grouped points index, [B, S, nsample]\n",
    "    \"\"\"\n",
    "    out = ball_query(new_xyz, xyz, K = nsample, radius = radius).idx\n",
    "    return out\n",
    "\n",
    "\n",
    "def sample_and_group(npoint, radius, nsample, xyz, points, returnfps=False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        npoint:\n",
    "        radius:\n",
    "        nsample:\n",
    "        xyz: input points position data, [B, N, 3]\n",
    "        points: input points data, [B, N, D]\n",
    "    Return:\n",
    "        new_xyz: sampled points position data, [B, npoint, nsample, 3]\n",
    "        new_points: sampled points data, [B, npoint, nsample, 3+D]\n",
    "    \"\"\"\n",
    "    B, N, C = xyz.shape\n",
    "    S = npoint\n",
    "    fps_idx = farthest_point_sample(xyz, npoint) # [B, npoint, C]\n",
    "    new_xyz = index_points(xyz, fps_idx)\n",
    "    idx = query_ball_point(radius, nsample, xyz, new_xyz)\n",
    "    grouped_xyz = index_points(xyz, idx) # [B, npoint, nsample, C]\n",
    "    grouped_xyz_norm = grouped_xyz - new_xyz.view(B, S, 1, C)\n",
    "\n",
    "    if points is not None:\n",
    "        grouped_points = index_points(points, idx)\n",
    "        new_points = torch.cat([grouped_xyz_norm, grouped_points], dim=-1) # [B, npoint, nsample, C+D]\n",
    "    else:\n",
    "        new_points = grouped_xyz_norm\n",
    "    if returnfps:\n",
    "        return new_xyz, new_points, grouped_xyz, fps_idx\n",
    "    else:\n",
    "        return new_xyz, new_points\n",
    "\n",
    "\n",
    "def sample_and_group_all(xyz, points):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: input points position data, [B, N, 3]\n",
    "        points: input points data, [B, N, D]\n",
    "    Return:\n",
    "        new_xyz: sampled points position data, [B, 1, 3]\n",
    "        new_points: sampled points data, [B, 1, N, 3+D]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    new_xyz = torch.zeros(B, 1, C).to(device)\n",
    "    grouped_xyz = xyz.view(B, 1, N, C)\n",
    "    if points is not None:\n",
    "        new_points = torch.cat([grouped_xyz, points.view(B, 1, N, -1)], dim=-1)\n",
    "    else:\n",
    "        new_points = grouped_xyz\n",
    "    return new_xyz, new_points\n",
    "\n",
    "\n",
    "class PointNetSetAbstraction(nn.Module):\n",
    "    def __init__(self, npoint, radius, nsample, in_channel, mlp, group_all):\n",
    "        super(PointNetSetAbstraction, self).__init__()\n",
    "        self.npoint = npoint\n",
    "        self.radius = radius\n",
    "        self.nsample = nsample\n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        last_channel = in_channel\n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm2d(out_channel))\n",
    "            last_channel = out_channel\n",
    "        self.group_all = group_all\n",
    "\n",
    "    def forward(self, xyz, points):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz: input points position data, [B, C, N]\n",
    "            points: input points data, [B, D, N]\n",
    "        Return:\n",
    "            new_xyz: sampled points position data, [B, C, S]\n",
    "            new_points_concat: sample points feature data, [B, D', S]\n",
    "        \"\"\"\n",
    "        xyz = xyz.permute(0, 2, 1)\n",
    "        if points is not None:\n",
    "            points = points.permute(0, 2, 1)\n",
    "\n",
    "        if self.group_all:\n",
    "            new_xyz, new_points = sample_and_group_all(xyz, points)\n",
    "        else:\n",
    "            new_xyz, new_points = sample_and_group(self.npoint, self.radius, self.nsample, xyz, points)\n",
    "        # new_xyz: sampled points position data, [B, npoint, C]\n",
    "        # new_points: sampled points data, [B, npoint, nsample, C+D]\n",
    "        new_points = new_points.permute(0, 3, 2, 1) # [B, C+D, nsample,npoint]\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            bn = self.mlp_bns[i]\n",
    "            new_points =  F.relu(bn(conv(new_points)))\n",
    "\n",
    "        new_points = torch.max(new_points, 2)[0]\n",
    "        new_xyz = new_xyz.permute(0, 2, 1)\n",
    "        return new_xyz, new_points\n",
    "\n",
    "\n",
    "class PointNetSetAbstractionMsg(nn.Module):\n",
    "    def __init__(self, npoint, radius_list, nsample_list, in_channel, mlp_list):\n",
    "        super(PointNetSetAbstractionMsg, self).__init__()\n",
    "        self.npoint = npoint\n",
    "        self.radius_list = radius_list\n",
    "        self.nsample_list = nsample_list\n",
    "        self.conv_blocks = nn.ModuleList()\n",
    "        self.bn_blocks = nn.ModuleList()\n",
    "        for i in range(len(mlp_list)):\n",
    "            convs = nn.ModuleList()\n",
    "            bns = nn.ModuleList()\n",
    "            last_channel = in_channel + 3\n",
    "            for out_channel in mlp_list[i]:\n",
    "                convs.append(nn.Conv2d(last_channel, out_channel, 1))\n",
    "                bns.append(nn.BatchNorm2d(out_channel))\n",
    "                last_channel = out_channel\n",
    "            self.conv_blocks.append(convs)\n",
    "            self.bn_blocks.append(bns)\n",
    "\n",
    "    def forward(self, xyz, points):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz: input points position data, [B, C, N]\n",
    "            points: input points data, [B, D, N]\n",
    "        Return:\n",
    "            new_xyz: sampled points position data, [B, C, S]\n",
    "            new_points_concat: sample points feature data, [B, D', S]\n",
    "        \"\"\"\n",
    "        xyz = xyz.permute(0, 2, 1)\n",
    "        if points is not None:\n",
    "            points = points.permute(0, 2, 1)\n",
    "\n",
    "        B, N, C = xyz.shape\n",
    "        S = self.npoint\n",
    "        new_xyz = index_points(xyz, farthest_point_sample(xyz, S))\n",
    "        new_points_list = []\n",
    "        for i, radius in enumerate(self.radius_list):\n",
    "            K = self.nsample_list[i]\n",
    "            group_idx = query_ball_point(radius, K, xyz, new_xyz)\n",
    "            grouped_xyz = index_points(xyz, group_idx)\n",
    "            grouped_xyz -= new_xyz.view(B, S, 1, C)\n",
    "            if points is not None:\n",
    "                grouped_points = index_points(points, group_idx)\n",
    "                grouped_points = torch.cat([grouped_points, grouped_xyz], dim=-1)\n",
    "            else:\n",
    "                grouped_points = grouped_xyz\n",
    "\n",
    "            grouped_points = grouped_points.permute(0, 3, 2, 1)  # [B, D, K, S]\n",
    "            for j in range(len(self.conv_blocks[i])):\n",
    "                conv = self.conv_blocks[i][j]\n",
    "                bn = self.bn_blocks[i][j]\n",
    "                grouped_points =  F.relu(bn(conv(grouped_points)))\n",
    "            new_points = torch.max(grouped_points, 2)[0]  # [B, D', S]\n",
    "            new_points_list.append(new_points)\n",
    "\n",
    "        new_xyz = new_xyz.permute(0, 2, 1)\n",
    "        new_points_concat = torch.cat(new_points_list, dim=1)\n",
    "        return new_xyz, new_points_concat\n",
    "\n",
    "\n",
    "class PointNetFeaturePropagation(nn.Module):\n",
    "    def __init__(self, in_channel, mlp):\n",
    "        super(PointNetFeaturePropagation, self).__init__()\n",
    "        self.mlp_convs = nn.ModuleList()\n",
    "        self.mlp_bns = nn.ModuleList()\n",
    "        last_channel = in_channel\n",
    "        for out_channel in mlp:\n",
    "            self.mlp_convs.append(nn.Conv1d(last_channel, out_channel, 1))\n",
    "            self.mlp_bns.append(nn.BatchNorm1d(out_channel))\n",
    "            last_channel = out_channel\n",
    "\n",
    "    def forward(self, xyz1, xyz2, points1, points2):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz1: input points position data, [B, C, N]\n",
    "            xyz2: sampled input points position data, [B, C, S]\n",
    "            points1: input points data, [B, D, N]\n",
    "            points2: input points data, [B, D, S]\n",
    "        Return:\n",
    "            new_points: upsampled points data, [B, D', N]\n",
    "        \"\"\"\n",
    "        xyz1 = xyz1.permute(0, 2, 1)\n",
    "        xyz2 = xyz2.permute(0, 2, 1)\n",
    "\n",
    "        points2 = points2.permute(0, 2, 1)\n",
    "        B, N, C = xyz1.shape\n",
    "        _, S, _ = xyz2.shape\n",
    "\n",
    "        if S == 1:\n",
    "            interpolated_points = points2.repeat(1, N, 1)\n",
    "        else:\n",
    "            dists = square_distance(xyz1, xyz2)\n",
    "            dists, idx = dists.sort(dim=-1)\n",
    "            dists, idx = dists[:, :, :3], idx[:, :, :3]  # [B, N, 3]\n",
    "\n",
    "            dist_recip = 1.0 / (dists + 1e-8)\n",
    "            norm = torch.sum(dist_recip, dim=2, keepdim=True)\n",
    "            weight = dist_recip / norm\n",
    "            interpolated_points = torch.sum(index_points(points2, idx) * weight.view(B, N, 3, 1), dim=2)\n",
    "\n",
    "        if points1 is not None:\n",
    "            points1 = points1.permute(0, 2, 1)\n",
    "            new_points = torch.cat([points1, interpolated_points], dim=-1)\n",
    "        else:\n",
    "            new_points = interpolated_points\n",
    "\n",
    "        new_points = new_points.permute(0, 2, 1)\n",
    "        for i, conv in enumerate(self.mlp_convs):\n",
    "            bn = self.mlp_bns[i]\n",
    "            new_points = F.relu(bn(conv(new_points)))\n",
    "        return new_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class get_model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(get_model, self).__init__()\n",
    "        self.sa1 = PointNetSetAbstraction(1024, 0.1, 32, 9 + 3, [32, 32, 64], False)\n",
    "        self.sa2 = PointNetSetAbstraction(256, 0.2, 32, 64 + 3, [64, 64, 128], False)\n",
    "        self.sa3 = PointNetSetAbstraction(64, 0.4, 32, 128 + 3, [128, 128, 256], False)\n",
    "        self.sa4 = PointNetSetAbstraction(16, 0.8, 32, 256 + 3, [256, 256, 512], False)\n",
    "        self.fp4 = PointNetFeaturePropagation(768, [256, 256])\n",
    "        self.fp3 = PointNetFeaturePropagation(384, [256, 256])\n",
    "        self.fp2 = PointNetFeaturePropagation(320, [256, 128])\n",
    "        self.fp1 = PointNetFeaturePropagation(128, [128, 128, 128])\n",
    "        self.conv1 = nn.Conv1d(128, 128, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.conv2 = nn.Conv1d(128, num_classes, 1)\n",
    "\n",
    "    def forward(self, xyz):\n",
    "        l0_points = xyz\n",
    "        l0_xyz = xyz[:,:3,:]\n",
    "\n",
    "        l1_xyz, l1_points = self.sa1(l0_xyz, l0_points)\n",
    "        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n",
    "        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n",
    "        l4_xyz, l4_points = self.sa4(l3_xyz, l3_points)\n",
    "\n",
    "        l3_points = self.fp4(l3_xyz, l4_xyz, l3_points, l4_points)\n",
    "        l2_points = self.fp3(l2_xyz, l3_xyz, l2_points, l3_points)\n",
    "        l1_points = self.fp2(l1_xyz, l2_xyz, l1_points, l2_points)\n",
    "        l0_points = self.fp1(l0_xyz, l1_xyz, None, l1_points)\n",
    "\n",
    "        x = self.drop1(F.relu(self.bn1(self.conv1(l0_points))))\n",
    "        x = self.conv2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x, l4_points\n",
    "\n",
    "\n",
    "class get_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(get_loss, self).__init__()\n",
    "    def forward(self, pred, target, trans_feat, weight):\n",
    "        total_loss = F.nll_loss(pred, target, weight=weight)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import  torch\n",
    "    model = get_model(13)\n",
    "    xyz = torch.rand(6, 9, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2048, 13]) torch.Size([6, 512, 16])\n"
     ]
    }
   ],
   "source": [
    "x, l4_points = model(xyz)\n",
    "print(x.shape, l4_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[-4.6364e-02, -5.2330e-03, -3.2129e-03,  ..., -3.0419e-02,\n",
       "            -3.8897e-02, -8.3740e-03],\n",
       "           [-5.9711e-02, -8.8515e-02, -4.9443e-02,  ..., -6.6577e-02,\n",
       "            -5.0221e-02, -2.6143e-02],\n",
       "           [-6.4317e-02, -1.1535e-02, -3.9840e-02,  ..., -3.0520e-02,\n",
       "             3.4140e-03, -3.7609e-02],\n",
       "           ...,\n",
       "           [-5.2618e-02, -8.2175e-02, -3.0063e-02,  ..., -6.2769e-02,\n",
       "            -5.0686e-02, -5.2275e-02],\n",
       "           [-2.4826e-02, -2.1310e-02, -2.8951e-02,  ..., -4.9462e-02,\n",
       "            -2.3668e-03, -5.1266e-02],\n",
       "           [-1.6182e-02, -5.7589e-02, -3.1335e-02,  ..., -7.7288e-02,\n",
       "            -6.7831e-02, -2.3971e-02]],\n",
       "\n",
       "          [[-5.9214e-02, -2.7416e-02,  1.4104e-03,  ..., -4.6262e-02,\n",
       "            -8.2154e-04, -2.7600e-02],\n",
       "           [-3.6558e-02, -8.0564e-02, -6.8477e-02,  ..., -5.3400e-03,\n",
       "            -7.1562e-02, -8.6677e-02],\n",
       "           [-5.3189e-02, -5.6111e-02, -4.9219e-02,  ..., -3.7220e-02,\n",
       "            -2.9924e-02, -1.0399e-01],\n",
       "           ...,\n",
       "           [ 5.8592e-03,  8.6690e-03, -2.6022e-02,  ..., -1.1932e-01,\n",
       "            -8.4644e-03, -1.0144e-01],\n",
       "           [-5.9254e-02, -2.8196e-02, -8.6847e-02,  ..., -8.6937e-02,\n",
       "            -7.5090e-02, -6.0550e-02],\n",
       "           [-3.0829e-02, -6.6950e-02, -3.0471e-02,  ..., -1.9860e-02,\n",
       "            -6.1785e-02, -5.4188e-02]],\n",
       "\n",
       "          [[-6.6204e-02, -8.9447e-03,  1.0572e-02,  ..., -5.2830e-02,\n",
       "            -1.9193e-02, -1.9448e-02],\n",
       "           [-6.9149e-02, -7.4444e-02, -7.6885e-03,  ..., -1.1733e-01,\n",
       "            -9.8076e-02, -6.9019e-02],\n",
       "           [-7.3468e-02, -2.1584e-02, -4.7308e-02,  ..., -9.4928e-02,\n",
       "            -3.1585e-02, -5.3808e-02],\n",
       "           ...,\n",
       "           [-6.3707e-02, -1.4452e-02, -6.4820e-02,  ..., -6.6910e-02,\n",
       "            -4.5108e-02, -6.0340e-02],\n",
       "           [-8.1790e-03, -7.7089e-02, -7.3495e-02,  ..., -3.9444e-02,\n",
       "            -3.5888e-02, -7.4132e-02],\n",
       "           [-4.2182e-02, -5.0602e-02, -8.2414e-02,  ..., -2.2664e-02,\n",
       "            -4.6324e-02, -6.9923e-02]]],\n",
       "\n",
       "\n",
       "         [[[ 7.3782e-02,  1.0271e-01,  3.9164e-02,  ...,  8.8553e-02,\n",
       "             7.1850e-02,  4.1304e-02],\n",
       "           [ 7.7837e-02,  1.2346e-01,  7.1125e-02,  ...,  8.3343e-02,\n",
       "             8.9816e-02,  1.1042e-01],\n",
       "           [ 7.6846e-02,  8.0510e-02,  7.3224e-02,  ...,  3.8250e-02,\n",
       "             1.1542e-01,  6.8010e-02],\n",
       "           ...,\n",
       "           [ 4.1614e-02,  5.5799e-02,  2.8557e-02,  ...,  1.2784e-01,\n",
       "             8.0430e-02,  7.9393e-02],\n",
       "           [ 5.3628e-02,  4.3069e-02,  9.5386e-02,  ...,  3.1189e-02,\n",
       "             4.6618e-02,  1.2189e-01],\n",
       "           [ 8.1422e-02,  5.0349e-02,  8.6172e-02,  ...,  1.3951e-01,\n",
       "             9.2355e-02,  7.7904e-02]],\n",
       "\n",
       "          [[ 7.5977e-02,  7.0030e-02,  6.1575e-02,  ...,  7.4097e-02,\n",
       "             1.0287e-01,  8.0327e-02],\n",
       "           [ 5.1560e-02,  1.1541e-01,  5.0972e-02,  ...,  1.1764e-01,\n",
       "             1.2055e-01,  9.5263e-02],\n",
       "           [ 1.0022e-01,  9.0210e-02,  8.0439e-02,  ...,  1.1747e-01,\n",
       "             6.8650e-02,  1.0061e-01],\n",
       "           ...,\n",
       "           [ 3.0001e-02,  5.8199e-02,  1.3457e-01,  ...,  8.5989e-02,\n",
       "             1.2393e-01,  6.5633e-02],\n",
       "           [ 8.9982e-02,  8.5374e-02,  1.2383e-01,  ...,  8.8004e-02,\n",
       "             5.4791e-02,  8.8830e-02],\n",
       "           [ 6.7363e-02,  5.2676e-02,  8.7731e-02,  ...,  3.4000e-02,\n",
       "             8.7870e-02,  6.5031e-02]],\n",
       "\n",
       "          [[ 8.8094e-02,  6.2556e-02,  4.7677e-02,  ...,  8.6191e-02,\n",
       "             9.1661e-02,  9.5777e-02],\n",
       "           [ 9.3498e-02,  8.8310e-02,  7.5978e-03,  ...,  6.5620e-02,\n",
       "             8.8846e-02,  9.6525e-02],\n",
       "           [ 8.5468e-02,  5.2957e-02,  5.3336e-02,  ...,  1.1322e-01,\n",
       "             8.1032e-02,  1.8084e-02],\n",
       "           ...,\n",
       "           [ 5.7529e-02,  3.3382e-02,  7.3612e-02,  ...,  1.1372e-01,\n",
       "             4.6868e-02,  4.8220e-02],\n",
       "           [ 6.5529e-02,  8.6518e-02,  1.0790e-01,  ...,  7.3208e-02,\n",
       "             2.3022e-02,  9.0714e-02],\n",
       "           [ 6.1951e-02,  5.5938e-02,  3.8064e-02,  ...,  9.3734e-02,\n",
       "             5.3289e-02,  8.1925e-02]]],\n",
       "\n",
       "\n",
       "         [[[-7.2479e-03,  9.2244e-03,  1.1118e-02,  ..., -3.4781e-03,\n",
       "             1.0094e-02, -3.5513e-02],\n",
       "           [ 1.4564e-02, -1.8613e-02, -2.2773e-02,  ...,  2.3344e-04,\n",
       "             6.2918e-03, -4.2445e-02],\n",
       "           [ 3.4199e-03,  1.2000e-02, -6.2862e-03,  ..., -6.9157e-03,\n",
       "             6.2171e-03, -3.3022e-02],\n",
       "           ...,\n",
       "           [-1.1484e-02,  9.3818e-03, -2.3006e-02,  ...,  3.0839e-02,\n",
       "             1.0073e-02, -6.7523e-02],\n",
       "           [-8.8180e-03,  2.5763e-02, -1.0046e-02,  ..., -5.5096e-03,\n",
       "             3.0537e-02, -1.0509e-02],\n",
       "           [-2.4142e-02, -2.4028e-02, -2.5536e-02,  ...,  2.6386e-02,\n",
       "            -2.1089e-02,  1.5399e-02]],\n",
       "\n",
       "          [[-1.9143e-02, -2.7140e-02, -1.1612e-03,  ..., -1.5304e-02,\n",
       "            -9.7545e-03,  3.5842e-02],\n",
       "           [-1.7976e-02, -5.7068e-02,  3.0651e-02,  ..., -3.2380e-03,\n",
       "            -8.3598e-02,  2.6420e-02],\n",
       "           [-1.9097e-02, -1.8678e-02,  5.9103e-02,  ..., -3.4756e-02,\n",
       "            -3.3727e-04,  1.1478e-02],\n",
       "           ...,\n",
       "           [-4.1118e-02, -2.0733e-02, -1.5158e-02,  ...,  5.8471e-02,\n",
       "            -7.2624e-02, -6.4261e-04],\n",
       "           [-3.3613e-02, -2.2229e-03,  3.9429e-02,  ...,  1.7831e-02,\n",
       "            -2.5939e-02,  1.6529e-02],\n",
       "           [-4.7951e-02, -5.4410e-03,  3.9831e-02,  ..., -4.0952e-02,\n",
       "             2.9674e-02, -2.8691e-02]],\n",
       "\n",
       "          [[-6.1933e-03,  1.0443e-02,  4.0816e-03,  ..., -1.6386e-02,\n",
       "            -3.2187e-02, -2.2141e-02],\n",
       "           [-2.2196e-03, -1.0132e-02,  1.2276e-02,  ...,  6.9994e-04,\n",
       "             4.8890e-02, -4.4911e-02],\n",
       "           [ 5.7646e-03,  9.5509e-03, -4.2200e-02,  ...,  3.2299e-02,\n",
       "             1.0496e-02,  1.1388e-02],\n",
       "           ...,\n",
       "           [-4.8964e-03, -3.0352e-02, -6.0146e-02,  ..., -6.5094e-03,\n",
       "             6.7831e-03, -4.4386e-02],\n",
       "           [-3.4903e-02, -1.1304e-03, -2.1583e-02,  ...,  1.6761e-02,\n",
       "            -3.2095e-02,  7.2878e-03],\n",
       "           [-5.5292e-03,  2.6662e-04, -1.4890e-03,  ...,  1.4144e-02,\n",
       "            -2.2843e-02, -6.8302e-03]]],\n",
       "\n",
       "\n",
       "         ...,\n",
       "\n",
       "\n",
       "         [[[ 6.1291e-02,  6.6212e-02,  5.0414e-02,  ...,  7.4403e-02,\n",
       "             4.6425e-02,  8.8982e-02],\n",
       "           [ 8.3942e-02,  9.3448e-02,  6.4323e-02,  ...,  7.4801e-02,\n",
       "             4.6382e-02,  4.1491e-02],\n",
       "           [ 3.2036e-02,  3.8425e-02,  1.7043e-02,  ...,  7.4997e-02,\n",
       "             7.1771e-03,  4.5530e-02],\n",
       "           ...,\n",
       "           [ 4.0653e-02,  8.8956e-02,  5.4736e-02,  ...,  3.7301e-02,\n",
       "             6.5421e-02,  5.5503e-02],\n",
       "           [ 2.8976e-02,  8.9379e-02,  6.9091e-02,  ...,  8.4850e-02,\n",
       "             4.9635e-02,  5.9959e-02],\n",
       "           [ 5.4022e-02,  6.6220e-02,  5.5945e-02,  ...,  3.6629e-02,\n",
       "             1.9683e-02,  1.4814e-02]],\n",
       "\n",
       "          [[ 9.6317e-02,  2.9534e-02,  1.1155e-01,  ...,  2.8157e-02,\n",
       "            -4.5720e-03,  7.3036e-02],\n",
       "           [ 8.2539e-02,  6.0582e-02,  1.0748e-01,  ...,  1.0719e-01,\n",
       "             2.2094e-02,  1.1142e-02],\n",
       "           [ 5.7584e-02, -3.5297e-03,  2.1418e-02,  ...,  4.7420e-02,\n",
       "             2.2169e-02,  5.3922e-03],\n",
       "           ...,\n",
       "           [ 7.1743e-02,  4.0989e-02,  2.3957e-02,  ..., -2.3722e-02,\n",
       "             4.0247e-02,  2.1163e-02],\n",
       "           [-4.8135e-03,  5.9359e-03,  8.1008e-02,  ...,  2.4482e-02,\n",
       "             2.7629e-02,  1.1357e-02],\n",
       "           [ 2.5045e-02,  2.1450e-02,  5.2424e-02,  ...,  2.9654e-02,\n",
       "             3.6575e-02,  3.6866e-03]],\n",
       "\n",
       "          [[ 6.6687e-02,  5.1265e-02,  2.4178e-02,  ...,  6.6951e-03,\n",
       "             6.2309e-02,  3.2473e-02],\n",
       "           [ 5.3469e-02,  2.1687e-02,  8.4931e-02,  ...,  5.6806e-02,\n",
       "            -1.1428e-02,  3.1304e-02],\n",
       "           [ 4.3666e-02,  5.7126e-02,  5.0635e-02,  ...,  3.4069e-02,\n",
       "             3.5343e-02,  6.0113e-02],\n",
       "           ...,\n",
       "           [ 6.7338e-02,  9.0019e-02,  6.4034e-02,  ...,  1.4745e-03,\n",
       "             5.5404e-02,  3.5002e-02],\n",
       "           [ 6.2657e-02,  5.1981e-02,  4.8605e-02,  ...,  8.2940e-02,\n",
       "             3.4514e-02,  2.9657e-02],\n",
       "           [ 7.9417e-02,  2.1406e-02,  4.0771e-02,  ...,  2.7905e-02,\n",
       "             5.2949e-02,  1.8028e-02]]],\n",
       "\n",
       "\n",
       "         [[[ 1.7141e-01,  1.2420e-01,  1.5156e-01,  ...,  1.5708e-01,\n",
       "             1.3481e-01,  1.4131e-01],\n",
       "           [ 1.2767e-01,  1.8079e-01,  1.8908e-01,  ...,  1.7898e-01,\n",
       "             1.9273e-01,  1.3286e-01],\n",
       "           [ 1.2532e-01,  1.8199e-01,  1.5953e-01,  ...,  1.8558e-01,\n",
       "             1.9485e-01,  1.5857e-01],\n",
       "           ...,\n",
       "           [ 1.7479e-01,  1.4479e-01,  2.1736e-01,  ...,  1.4121e-01,\n",
       "             2.0787e-01,  1.4759e-01],\n",
       "           [ 1.5186e-01,  1.5781e-01,  1.5259e-01,  ...,  1.9002e-01,\n",
       "             1.4170e-01,  1.6161e-01],\n",
       "           [ 1.4898e-01,  1.3044e-01,  1.3641e-01,  ...,  1.2446e-01,\n",
       "             1.3266e-01,  1.0072e-01]],\n",
       "\n",
       "          [[ 1.3916e-01,  1.3700e-01,  2.1373e-01,  ...,  1.8957e-01,\n",
       "             1.5806e-01,  2.0491e-01],\n",
       "           [ 1.3245e-01,  1.8722e-01,  1.5797e-01,  ...,  1.2556e-01,\n",
       "             1.6223e-01,  1.9055e-01],\n",
       "           [ 1.5660e-01,  1.6678e-01,  1.8430e-01,  ...,  2.0601e-01,\n",
       "             1.5424e-01,  1.8528e-01],\n",
       "           ...,\n",
       "           [ 1.5467e-01,  1.8007e-01,  1.8120e-01,  ...,  1.7444e-01,\n",
       "             2.3785e-01,  1.6986e-01],\n",
       "           [ 1.6148e-01,  1.6924e-01,  1.6926e-01,  ...,  2.0946e-01,\n",
       "             1.2001e-01,  1.4044e-01],\n",
       "           [ 1.0520e-01,  1.1675e-01,  1.1701e-01,  ...,  1.4564e-01,\n",
       "             1.2724e-01,  1.5154e-01]],\n",
       "\n",
       "          [[ 1.5497e-01,  1.2635e-01,  1.4439e-01,  ...,  1.5912e-01,\n",
       "             1.2466e-01,  1.5685e-01],\n",
       "           [ 1.4684e-01,  1.7198e-01,  1.1326e-01,  ...,  1.8475e-01,\n",
       "             1.9874e-01,  1.5680e-01],\n",
       "           [ 1.3153e-01,  1.3283e-01,  1.6327e-01,  ...,  2.1243e-01,\n",
       "             1.8775e-01,  1.5768e-01],\n",
       "           ...,\n",
       "           [ 1.3227e-01,  1.2697e-01,  1.6780e-01,  ...,  1.5999e-01,\n",
       "             1.4849e-01,  1.5403e-01],\n",
       "           [ 1.3899e-01,  1.3333e-01,  1.8811e-01,  ...,  1.5714e-01,\n",
       "             1.3628e-01,  1.8108e-01],\n",
       "           [ 1.2341e-01,  1.3791e-01,  1.6498e-01,  ...,  1.6751e-01,\n",
       "             1.0872e-01,  1.6058e-01]]],\n",
       "\n",
       "\n",
       "         [[[ 5.6760e-02, -7.1454e-02,  3.4979e-02,  ...,  9.7858e-04,\n",
       "            -1.6170e-02, -1.8609e-02],\n",
       "           [ 2.0363e-02, -3.2438e-02,  2.0563e-02,  ..., -4.9761e-02,\n",
       "             3.8575e-02, -6.5036e-04],\n",
       "           [ 3.0811e-02,  4.2594e-02,  1.1407e-02,  ...,  1.5357e-02,\n",
       "             1.6589e-02, -3.0119e-02],\n",
       "           ...,\n",
       "           [ 8.6688e-02, -3.1383e-03,  1.9992e-02,  ...,  6.3775e-02,\n",
       "             4.7774e-02,  3.2577e-02],\n",
       "           [ 3.7582e-02,  1.0679e-02, -5.0043e-02,  ...,  3.5444e-02,\n",
       "             9.6218e-02,  2.7660e-02],\n",
       "           [ 9.0415e-03,  2.1325e-02, -6.0961e-03,  ..., -5.7144e-02,\n",
       "             1.0086e-01, -1.1545e-03]],\n",
       "\n",
       "          [[ 1.9347e-02, -3.4959e-02,  2.1737e-02,  ...,  2.8690e-02,\n",
       "             1.3617e-02,  1.5220e-02],\n",
       "           [ 2.2155e-02, -6.1131e-02,  8.0432e-02,  ...,  1.0401e-01,\n",
       "             3.4620e-02,  6.4782e-02],\n",
       "           [ 6.1287e-02, -5.9050e-02, -4.3562e-02,  ...,  1.2733e-01,\n",
       "             5.5949e-02, -4.2099e-02],\n",
       "           ...,\n",
       "           [ 5.5221e-02,  8.1727e-02,  9.0495e-02,  ..., -3.5723e-02,\n",
       "             7.7637e-02,  4.2123e-02],\n",
       "           [ 3.1810e-02,  1.1128e-02,  1.0115e-01,  ..., -4.1752e-02,\n",
       "             8.6621e-02, -8.7245e-03],\n",
       "           [ 4.2449e-02,  6.8862e-02,  1.3159e-01,  ...,  6.6714e-02,\n",
       "             3.0161e-02,  3.0252e-02]],\n",
       "\n",
       "          [[ 1.0092e-02,  1.2548e-02,  1.5950e-02,  ...,  3.0712e-02,\n",
       "             2.6460e-02, -4.7034e-03],\n",
       "           [ 1.5831e-03,  1.1060e-01, -1.3608e-02,  ..., -1.7661e-03,\n",
       "             9.5412e-02,  3.4949e-02],\n",
       "           [ 1.3231e-03, -4.6972e-03,  6.8716e-02,  ..., -3.5620e-02,\n",
       "             1.4348e-01, -4.9581e-04],\n",
       "           ...,\n",
       "           [ 2.9678e-02,  7.2815e-02,  5.3633e-02,  ...,  1.8480e-02,\n",
       "             5.3460e-02,  8.3314e-02],\n",
       "           [ 7.0605e-04, -2.6404e-03,  3.7125e-02,  ...,  2.9209e-02,\n",
       "             3.8802e-02,  1.2676e-01],\n",
       "           [ 3.5882e-02,  7.8906e-02,  5.6881e-02,  ...,  1.3579e-02,\n",
       "             5.5423e-02,  3.4909e-02]]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.unet_v3 import unet_water\n",
    "\n",
    "import torch\n",
    "a = unet_water(embedding_input=0)\n",
    "b = torch.rand((1, 1, 10, 100, 100))\n",
    "a(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
